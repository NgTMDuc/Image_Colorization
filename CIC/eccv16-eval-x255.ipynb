{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nimport warnings\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-13T11:54:22.311915Z","iopub.execute_input":"2023-07-13T11:54:22.312390Z","iopub.status.idle":"2023-07-13T11:54:22.319853Z","shell.execute_reply.started":"2023-07-13T11:54:22.312352Z","shell.execute_reply":"2023-07-13T11:54:22.318869Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"images_paths = list()\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        image_path = os.path.join(dirname, filename)\n        if image_path.endswith('.jpg') or image_path.endswith(\".png\"):\n            images_paths.append(image_path)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:54:22.322000Z","iopub.execute_input":"2023-07-13T11:54:22.322661Z","iopub.status.idle":"2023-07-13T11:54:25.675511Z","shell.execute_reply.started":"2023-07-13T11:54:22.322628Z","shell.execute_reply":"2023-07-13T11:54:25.674462Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class ColorizationDataset(Dataset):\n    def __init__(self, paths):\n        self.paths = paths\n\n    def __getitem__(self, idx):\n        return self.paths[idx]\n\n    def __len__(self):\n        return len(self.paths)\n    \n    \ndef make_dataloader(batch_size=16, n_workers=2, pin_memory=True, **kwargs):\n    dataset = ColorizationDataset(**kwargs)\n    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=n_workers,\n                            pin_memory=pin_memory)\n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:54:25.677121Z","iopub.execute_input":"2023-07-13T11:54:25.677514Z","iopub.status.idle":"2023-07-13T11:54:25.684948Z","shell.execute_reply.started":"2023-07-13T11:54:25.677478Z","shell.execute_reply":"2023-07-13T11:54:25.683996Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"np.random.seed(123)\nall_images = images_paths\ntrain_range = int(0.8 * len(all_images))\nrand_idxs = np.random.permutation(len(all_images))\ntrain_idxs = rand_idxs[:train_range] \nval_idxs = rand_idxs[train_range:] \ntrain_paths = [all_images[x] for x in train_idxs]\nval_paths = [all_images[x] for x in val_idxs]\nprint(len(train_paths), len(val_paths))","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:54:25.688006Z","iopub.execute_input":"2023-07-13T11:54:25.688740Z","iopub.status.idle":"2023-07-13T11:54:25.706137Z","shell.execute_reply.started":"2023-07-13T11:54:25.688703Z","shell.execute_reply":"2023-07-13T11:54:25.704884Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"11019 2755\n","output_type":"stream"}]},{"cell_type":"code","source":"train_loader = make_dataloader(paths=train_paths)\nval_loader = make_dataloader(batch_size=32, paths=val_paths)  # INCREASED\nlen(train_loader), len(val_loader)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:54:25.708066Z","iopub.execute_input":"2023-07-13T11:54:25.708848Z","iopub.status.idle":"2023-07-13T11:54:25.730500Z","shell.execute_reply.started":"2023-07-13T11:54:25.708799Z","shell.execute_reply":"2023-07-13T11:54:25.729549Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"(689, 87)"},"metadata":{}}]},{"cell_type":"code","source":"def get_name_from_path(path):\n    return path.split('/')[-1]","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:54:25.732203Z","iopub.execute_input":"2023-07-13T11:54:25.732999Z","iopub.status.idle":"2023-07-13T11:54:25.743207Z","shell.execute_reply.started":"2023-07-13T11:54:25.732963Z","shell.execute_reply":"2023-07-13T11:54:25.742251Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def save_grayscale_batch(batch):\n    for img_path in batch:\n        img_rgb = Image.open(img_path)\n        img_gray = img_rgb.convert('L')\n        img_gray.save(f'{get_name_from_path(img_path)}')","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:54:25.745121Z","iopub.execute_input":"2023-07-13T11:54:25.745844Z","iopub.status.idle":"2023-07-13T11:54:25.759210Z","shell.execute_reply.started":"2023-07-13T11:54:25.745798Z","shell.execute_reply":"2023-07-13T11:54:25.758268Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def save_batch(batch):\n    for img_path in batch:\n        img_rgb = Image.open(img_path)\n        img_rgb.save(f'{get_name_from_path(img_path)}')","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:54:25.760866Z","iopub.execute_input":"2023-07-13T11:54:25.761579Z","iopub.status.idle":"2023-07-13T11:54:25.773595Z","shell.execute_reply.started":"2023-07-13T11:54:25.761545Z","shell.execute_reply":"2023-07-13T11:54:25.772612Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def load_grayscale_batch(batch):\n    return [f'{get_name_from_path(img_path)}' for img_path in batch]","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:54:25.777704Z","iopub.execute_input":"2023-07-13T11:54:25.778176Z","iopub.status.idle":"2023-07-13T11:54:25.786385Z","shell.execute_reply.started":"2023-07-13T11:54:25.778151Z","shell.execute_reply":"2023-07-13T11:54:25.785385Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def del_grayscale_batch(batch):\n    for img_path in batch:\n        os.remove(f'{get_name_from_path(img_path)}')","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:54:25.788277Z","iopub.execute_input":"2023-07-13T11:54:25.789024Z","iopub.status.idle":"2023-07-13T11:54:25.799595Z","shell.execute_reply.started":"2023-07-13T11:54:25.788990Z","shell.execute_reply":"2023-07-13T11:54:25.798557Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\nclass BaseColor(nn.Module):\n    def __init__(self):\n        super(BaseColor, self).__init__()\n\n        self.l_cent = 50.\n        self.l_norm = 100.\n        self.ab_norm = 110.\n\n    def normalize_l(self, in_l):\n        return (in_l-self.l_cent)/self.l_norm\n\n    def unnormalize_l(self, in_l):\n        return in_l*self.l_norm + self.l_cent\n\n    def normalize_ab(self, in_ab):\n        return in_ab/self.ab_norm\n\n    def unnormalize_ab(self, in_ab):\n        return in_ab*self.ab_norm\n\n\n\nclass ECCVGenerator(BaseColor):\n    def __init__(self, norm_layer=nn.BatchNorm2d):\n        super(ECCVGenerator, self).__init__()\n\n        model1=[nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=True),]\n        model1+=[nn.ReLU(True),]\n        model1+=[nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=True),]\n        model1+=[nn.ReLU(True),]\n        model1+=[norm_layer(64),]\n\n        model2=[nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=True),]\n        model2+=[nn.ReLU(True),]\n        model2+=[nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1, bias=True),]\n        model2+=[nn.ReLU(True),]\n        model2+=[norm_layer(128),]\n\n        model3=[nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n        model3+=[nn.ReLU(True),]\n        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n        model3+=[nn.ReLU(True),]\n        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1, bias=True),]\n        model3+=[nn.ReLU(True),]\n        model3+=[norm_layer(256),]\n\n        model4=[nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n        model4+=[nn.ReLU(True),]\n        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n        model4+=[nn.ReLU(True),]\n        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n        model4+=[nn.ReLU(True),]\n        model4+=[norm_layer(512),]\n\n        model5=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n        model5+=[nn.ReLU(True),]\n        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n        model5+=[nn.ReLU(True),]\n        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n        model5+=[nn.ReLU(True),]\n        model5+=[norm_layer(512),]\n\n        model6=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n        model6+=[nn.ReLU(True),]\n        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n        model6+=[nn.ReLU(True),]\n        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n        model6+=[nn.ReLU(True),]\n        model6+=[norm_layer(512),]\n\n        model7=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n        model7+=[nn.ReLU(True),]\n        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n        model7+=[nn.ReLU(True),]\n        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n        model7+=[nn.ReLU(True),]\n        model7+=[norm_layer(512),]\n\n        model8=[nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=True),]\n        model8+=[nn.ReLU(True),]\n        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n        model8+=[nn.ReLU(True),]\n        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n        model8+=[nn.ReLU(True),]\n\n        model8+=[nn.Conv2d(256, 313, kernel_size=1, stride=1, padding=0, bias=True),]\n\n        self.model1 = nn.Sequential(*model1)\n        self.model2 = nn.Sequential(*model2)\n        self.model3 = nn.Sequential(*model3)\n        self.model4 = nn.Sequential(*model4)\n        self.model5 = nn.Sequential(*model5)\n        self.model6 = nn.Sequential(*model6)\n        self.model7 = nn.Sequential(*model7)\n        self.model8 = nn.Sequential(*model8)\n\n        self.softmax = nn.Softmax(dim=1)\n        self.model_out = nn.Conv2d(313, 2, kernel_size=1, padding=0, dilation=1, stride=1, bias=False)\n        self.upsample4 = nn.Upsample(scale_factor=4, mode='bilinear')\n\n    def forward(self, input_l):\n        conv1_2 = self.model1(self.normalize_l(input_l))\n        conv2_2 = self.model2(conv1_2)\n        conv3_3 = self.model3(conv2_2)\n        conv4_3 = self.model4(conv3_3)\n        conv5_3 = self.model5(conv4_3)\n        conv6_3 = self.model6(conv5_3)\n        conv7_3 = self.model7(conv6_3)\n        conv8_3 = self.model8(conv7_3)\n        out_reg = self.model_out(self.softmax(conv8_3))\n\n        return self.unnormalize_ab(self.upsample4(out_reg))\n\ndef eccv16(pretrained=True):\n    model = ECCVGenerator()\n    if(pretrained):\n        import torch.utils.model_zoo as model_zoo\n        model.load_state_dict(model_zoo.load_url('https://colorizers.s3.us-east-2.amazonaws.com/colorization_release_v2-9b330a0b.pth',map_location='cpu',check_hash=True))\n    return model\n\n\nfrom PIL import Image\nimport numpy as np\nfrom skimage import color\nimport torch\nimport torch.nn.functional as F\n\n# def load_img(img_path):\ndef load_imgs(img_path_list):\n    out_np_list = [np.asarray(Image.open(img_path).convert('RGB'))\n                   for img_path in img_path_list]\n    \n    # if(out_np.ndim==2):\n    #    out_np = np.tile(out_np[:,:,None],3)\n    for i, out_np in enumerate(out_np_list):\n        if out_np.ndim==2:\n            out_np_list[i] = np.tile(out_np[:,:,None],3)\n    # return out_np\n    return out_np_list\n\ndef resize_img(img, HW=(256,256), resample=3):\n    return np.asarray(Image.fromarray(img).resize((HW[1],HW[0]), resample=resample))\n\n# def preprocess_img(img_rgb_orig, HW=(256,256), resample=3):\ndef preprocess_imgs(img_rgb_orig_list, HW=(256,256), resample=3):\n    # return original size L and resized L as torch Tensors\n    # img_rgb_rs = resize_img(img_rgb_orig, HW=HW, resample=resample)\n    img_rgb_rs_list = [resize_img(img_rgb_orig, HW=HW, resample=resample)\n                       for img_rgb_orig in img_rgb_orig_list]\n\n    # img_lab_orig = color.rgb2lab(img_rgb_orig)\n    # img_lab_rs = color.rgb2lab(img_rgb_rs)\n    img_lab_orig_list = [color.rgb2lab(img_rgb_orig)\n                         for img_rgb_orig in img_rgb_orig_list]\n    img_lab_rs_list = [color.rgb2lab(img_rgb_rs)\n                       for img_rgb_rs in img_rgb_rs_list]\n\n    # img_l_orig = img_lab_orig[:,:,0]\n    # img_l_rs = img_lab_rs[:,:,0]\n    img_l_orig_list = [img_lab_orig[:,:,0]\n                       for img_lab_orig in img_lab_orig_list]\n    img_l_rs_list = [img_lab_rs[:,:,0]\n                     for img_lab_rs in img_lab_rs_list]\n    # FOR GT\n    # (256, 256, 2) 1 2 -> 0 1\n    img_ab_rs_list = [torch.transpose(torch.transpose(torch.Tensor(img_lab_rs[:,:,1:3]), 1, 2), 0, 1)\n                     for img_lab_rs in img_lab_rs_list]\n\n    # RESHAPE FIRST\n    img_l_rs_list_reshaped = [torch.Tensor(img_l_rs)[None, None, :, :]\n                              for img_l_rs in img_l_rs_list]\n    img_ab_rs_list_reshaped = [torch.Tensor(img_ab_rs)[None,:, :, :]\n                              for img_ab_rs in img_ab_rs_list]\n\n    # tens_orig_l = torch.Tensor(img_l_orig)[None,None,:,:]\n    # tens_rs_l = torch.Tensor(img_l_rs)[None,None,:,:]\n    tens_orig_l_list = [torch.Tensor(img_l_orig)[None,None,:,:]\n                        for img_l_orig in img_l_orig_list]\n    \n    batch_size = len(img_rgb_orig_list)\n    tens_rs_l_tensor = torch.cat(img_l_rs_list_reshaped, dim=0)\n    tens_rs_ab_tensor = torch.cat(img_ab_rs_list_reshaped, dim=0)\n\n    return (tens_orig_l_list, tens_rs_l_tensor, tens_rs_ab_tensor)\n\n# def postprocess_tens(tens_orig_l, out_ab, mode='bilinear'):\ndef postprocess_imgs(tens_orig_l_list, out_ab, mode='bilinear'):\n    # tens_orig_l     1 x 1 x H_orig x W_orig\n    # out_ab         1 x 2 x H x W\n    \n    # tens_orig_l_list: (16 elements) 1 x 1 x H_orig x W_orig\n    # out_ab                          16 x 2 x H x W\n\n    # HW_orig = tens_orig_l.shape[2:]\n    # HW = out_ab.shape[2:]\n    HW_orig_list = [tens_orig_l.shape[2:] for tens_orig_l in tens_orig_l_list]  # 16 ele H_orig x W_orig\n    HW = out_ab.shape[2:]  # (H, W)\n\n    # call resize function if needed\n    #if(HW_orig[0]!=HW[0] or HW_orig[1]!=HW[1]):\n    #    out_ab_orig = F.interpolate(out_ab, size=HW_orig, mode='bilinear')\n    #else:\n    #    out_ab_orig = out_ab\n    \n    out_ab_orig_list = list()\n    for i, HW_orig in enumerate(HW_orig_list):\n        if(HW_orig[0]!=HW[0] or HW_orig[1]!=HW[1]):\n            out_ab_orig = F.interpolate(out_ab[i][None, :, :, :], size=HW_orig, mode='bilinear')\n        else:\n            out_ab_orig = out_ab[i][None, :, :, :]     \n        out_ab_orig_list.append(out_ab_orig)\n\n    # out_lab_orig = torch.cat((tens_orig_l, out_ab_orig), dim=1)\n    out_lab_orig_list = list()\n    for i in range(len(out_ab_orig_list)):\n        out_lab_orig_list.append(\n            torch.cat((tens_orig_l_list[i].cuda(), out_ab_orig_list[i]), dim=1)\n        )\n    \n    # return color.lab2rgb(out_lab_orig.data.cpu().numpy()[0,...].transpose((1,2,0)))\n    return [color.lab2rgb(out_lab_orig.data.cpu().numpy()[0,...].transpose((1,2,0)))\n           for out_lab_orig in out_lab_orig_list]","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:54:25.801274Z","iopub.execute_input":"2023-07-13T11:54:25.802168Z","iopub.status.idle":"2023-07-13T11:54:25.852992Z","shell.execute_reply.started":"2023-07-13T11:54:25.802131Z","shell.execute_reply":"2023-07-13T11:54:25.851870Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"model = eccv16(pretrained=True).cuda()\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:54:25.854506Z","iopub.execute_input":"2023-07-13T11:54:25.855725Z","iopub.status.idle":"2023-07-13T11:54:26.308076Z","shell.execute_reply.started":"2023-07-13T11:54:25.855684Z","shell.execute_reply":"2023-07-13T11:54:26.307007Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"ECCVGenerator(\n  (model1): Sequential(\n    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (model2): Sequential(\n    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (model3): Sequential(\n    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (5): ReLU(inplace=True)\n    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (model4): Sequential(\n    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): ReLU(inplace=True)\n    (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (model5): Sequential(\n    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n    (3): ReLU(inplace=True)\n    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n    (5): ReLU(inplace=True)\n    (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (model6): Sequential(\n    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n    (3): ReLU(inplace=True)\n    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n    (5): ReLU(inplace=True)\n    (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (model7): Sequential(\n    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): ReLU(inplace=True)\n    (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (model8): Sequential(\n    (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): ReLU(inplace=True)\n    (6): Conv2d(256, 313, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (softmax): Softmax(dim=1)\n  (model_out): Conv2d(313, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (upsample4): Upsample(scale_factor=4.0, mode='bilinear')\n)"},"metadata":{}}]},{"cell_type":"code","source":"import math\nfrom PIL import Image\nimport cv2\nfrom cv2 import imshow\n\ndef gaussian(window_size, sigma):\n    gauss = torch.Tensor([math.exp(-(x - window_size//2) ** 2/float(2*sigma**2)) for x in range(window_size)])\n    return gauss/gauss.sum()\n\n\ndef create_window(window_size, channel=1):\n\n    # Generate an 1D tensor containing values sampled from a gaussian distribution\n    _1d_window = gaussian(window_size=window_size, sigma=1.5).unsqueeze(1)\n    \n    # Converting to 2D  \n    _2d_window = _1d_window.mm(_1d_window.t()).float().unsqueeze(0).unsqueeze(0)\n     \n    window = torch.Tensor(_2d_window.expand(channel, 1, window_size, window_size).contiguous())\n\n    return window\n\n\ndef ssim(img1, img2, val_range, window_size=11, window=None, size_average=True, full=False):\n\n    L = val_range # L is the dynamic range of the pixel values (255 for 8-bit grayscale images),\n\n    pad = window_size // 2\n    \n    try:\n        _, channels, height, width = img1.size()\n    except:\n        channels, height, width = img1.size()\n\n    # if window is not provided, init one\n    if window is None: \n        real_size = min(window_size, height, width) # window should be atleast 11x11 \n        window = create_window(real_size, channel=channels).to(img1.device)\n    \n    # calculating the mu parameter (locally) for both images using a gaussian filter \n    # calculates the luminosity params\n    mu1 = F.conv2d(img1, window, padding=pad, groups=channels)\n    mu2 = F.conv2d(img2, window, padding=pad, groups=channels)\n    \n    mu1_sq = mu1 ** 2\n    mu2_sq = mu2 ** 2 \n    mu12 = mu1 * mu2\n\n    # now we calculate the sigma square parameter\n    # Sigma deals with the contrast component \n    sigma1_sq = F.conv2d(img1 * img1, window, padding=pad, groups=channels) - mu1_sq\n    sigma2_sq = F.conv2d(img2 * img2, window, padding=pad, groups=channels) - mu2_sq\n    sigma12 =  F.conv2d(img1 * img2, window, padding=pad, groups=channels) - mu12\n\n    # Some constants for stability \n    C1 = (0.01 * 255) ** 2  # NOTE: Removed L from here (ref PT implementation)\n    C2 = (0.03 * 255) ** 2 \n\n    contrast_metric = (2.0 * sigma12 + C2) / (sigma1_sq + sigma2_sq + C2)\n    contrast_metric = torch.mean(contrast_metric)\n\n    numerator1 = 2 * mu12 + C1  \n    numerator2 = 2 * sigma12 + C2\n    denominator1 = mu1_sq + mu2_sq + C1 \n    denominator2 = sigma1_sq + sigma2_sq + C2\n\n    ssim_score = (numerator1 * numerator2) / (denominator1 * denominator2)\n\n    if size_average:\n        ret = ssim_score.mean() \n    else: \n        ret = ssim_score.mean(1).mean(1).mean(1)\n    \n    if full:\n        return ret, contrast_metric\n    \n    return ret\n\n\n# display imgs \ndef display_imgs(x, transpose=True, resize=True):\n    if resize:\n        x=cv2.resize(x, (400, 400))\n    if transpose:\n        imshow(cv2.cvtColor(x, cv2.COLOR_BGR2RGB))\n    else:\n        imshow(x)\n\ndef tensorify(x, mul = True):\n#   if vector is already norm, set mul = False\n    if mul:\n        return torch.Tensor(x.transpose((2, 0, 1))).unsqueeze(0).float().div(255.0)\n    else:\n        return torch.Tensor(x.transpose((2, 0, 1))).unsqueeze(0).float()\n\ndef compute_ssim(image1, image2):\n    img1 = tensorify(np.asarray(image1))\n    img2 = tensorify(np.asarray(image2))\n    return ssim(img1, img2, val_range= 255)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:54:26.309709Z","iopub.execute_input":"2023-07-13T11:54:26.310110Z","iopub.status.idle":"2023-07-13T11:54:26.331781Z","shell.execute_reply.started":"2023-07-13T11:54:26.310075Z","shell.execute_reply":"2023-07-13T11:54:26.330707Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"import gc","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:54:26.333452Z","iopub.execute_input":"2023-07-13T11:54:26.333867Z","iopub.status.idle":"2023-07-13T11:54:26.346692Z","shell.execute_reply.started":"2023-07-13T11:54:26.333796Z","shell.execute_reply":"2023-07-13T11:54:26.345370Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"all_scores = list()\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n\n    for batch_idx, batch in enumerate(val_loader):\n        original_imgs = load_imgs(batch)\n        orig_l_list, rs_l_tensor, gt = preprocess_imgs(original_imgs)\n\n        output_tensor = model(rs_l_tensor.cuda()) \n\n        \n        postprocessed_imgs = postprocess_imgs(orig_l_list, output_tensor)\n\n        scores = [compute_ssim(postprocessed_imgs[i]*255, np.asarray(Image.open(batch[i]).convert(\"RGB\")))\n                  for i in range(len(batch))]\n        all_scores.extend(scores)\n        print(np.mean(scores), '\\t', f'Current accumulative score={np.mean(all_scores):.15f}')\n        \n        gc.collect()\n        torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:54:26.348698Z","iopub.execute_input":"2023-07-13T11:54:26.349076Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"0.99923736 \t Current accumulative score=0.999237358570099\n0.9991261 \t Current accumulative score=0.999181687831879\n0.9992622 \t Current accumulative score=0.999208509922028\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}