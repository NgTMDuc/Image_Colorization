{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c84a0f4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-07-12T20:17:53.743868Z",
     "iopub.status.busy": "2023-07-12T20:17:53.743520Z",
     "iopub.status.idle": "2023-07-12T20:17:57.514270Z",
     "shell.execute_reply": "2023-07-12T20:17:57.512245Z"
    },
    "papermill": {
     "duration": 3.780325,
     "end_time": "2023-07-12T20:17:57.517402",
     "exception": false,
     "start_time": "2023-07-12T20:17:53.737077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c1c93a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-12T20:17:57.532929Z",
     "iopub.status.busy": "2023-07-12T20:17:57.532254Z",
     "iopub.status.idle": "2023-07-12T20:18:05.384535Z",
     "shell.execute_reply": "2023-07-12T20:18:05.383538Z"
    },
    "papermill": {
     "duration": 7.862179,
     "end_time": "2023-07-12T20:18:05.386889",
     "exception": false,
     "start_time": "2023-07-12T20:17:57.524710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "images_paths = list()\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        image_path = os.path.join(dirname, filename)\n",
    "        if image_path.endswith('.jpg') or image_path.endswith(\".png\"):\n",
    "            images_paths.append(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1523fc5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-12T20:18:05.398952Z",
     "iopub.status.busy": "2023-07-12T20:18:05.397463Z",
     "iopub.status.idle": "2023-07-12T20:18:05.405229Z",
     "shell.execute_reply": "2023-07-12T20:18:05.404422Z"
    },
    "papermill": {
     "duration": 0.015339,
     "end_time": "2023-07-12T20:18:05.407140",
     "exception": false,
     "start_time": "2023-07-12T20:18:05.391801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, paths):\n",
    "        self.paths = paths\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.paths[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    \n",
    "def make_dataloader(batch_size=16, n_workers=2, pin_memory=True, **kwargs):\n",
    "    dataset = ColorizationDataset(**kwargs)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=n_workers,\n",
    "                            pin_memory=pin_memory)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd97fe0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-12T20:18:05.417840Z",
     "iopub.status.busy": "2023-07-12T20:18:05.417561Z",
     "iopub.status.idle": "2023-07-12T20:18:05.427814Z",
     "shell.execute_reply": "2023-07-12T20:18:05.426797Z"
    },
    "papermill": {
     "duration": 0.019652,
     "end_time": "2023-07-12T20:18:05.431422",
     "exception": false,
     "start_time": "2023-07-12T20:18:05.411770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11019 2755\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "all_images = images_paths\n",
    "train_range = int(0.8 * len(all_images))\n",
    "rand_idxs = np.random.permutation(len(all_images))\n",
    "train_idxs = rand_idxs[:train_range] \n",
    "val_idxs = rand_idxs[train_range:] \n",
    "train_paths = [all_images[x] for x in train_idxs]\n",
    "val_paths = [all_images[x] for x in val_idxs]\n",
    "print(len(train_paths), len(val_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc453206",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-12T20:18:05.442560Z",
     "iopub.status.busy": "2023-07-12T20:18:05.441656Z",
     "iopub.status.idle": "2023-07-12T20:18:05.449575Z",
     "shell.execute_reply": "2023-07-12T20:18:05.448670Z"
    },
    "papermill": {
     "duration": 0.015385,
     "end_time": "2023-07-12T20:18:05.451480",
     "exception": false,
     "start_time": "2023-07-12T20:18:05.436095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(689, 173)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = make_dataloader(paths=train_paths)\n",
    "val_loader = make_dataloader(paths=val_paths)\n",
    "len(train_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2428c298",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-12T20:18:05.462509Z",
     "iopub.status.busy": "2023-07-12T20:18:05.461744Z",
     "iopub.status.idle": "2023-07-12T20:18:05.466695Z",
     "shell.execute_reply": "2023-07-12T20:18:05.465816Z"
    },
    "papermill": {
     "duration": 0.012433,
     "end_time": "2023-07-12T20:18:05.468592",
     "exception": false,
     "start_time": "2023-07-12T20:18:05.456159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_name_from_path(path):\n",
    "    return path.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfed24e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-12T20:18:05.479846Z",
     "iopub.status.busy": "2023-07-12T20:18:05.478925Z",
     "iopub.status.idle": "2023-07-12T20:18:05.484259Z",
     "shell.execute_reply": "2023-07-12T20:18:05.483363Z"
    },
    "papermill": {
     "duration": 0.012744,
     "end_time": "2023-07-12T20:18:05.486147",
     "exception": false,
     "start_time": "2023-07-12T20:18:05.473403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_grayscale_batch(batch):\n",
    "    for img_path in batch:\n",
    "        img_rgb = Image.open(img_path)\n",
    "        img_gray = img_rgb.convert('L')\n",
    "        img_gray.save(f'{get_name_from_path(img_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "519cf0e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-12T20:18:05.497167Z",
     "iopub.status.busy": "2023-07-12T20:18:05.496420Z",
     "iopub.status.idle": "2023-07-12T20:18:05.501030Z",
     "shell.execute_reply": "2023-07-12T20:18:05.500093Z"
    },
    "papermill": {
     "duration": 0.012106,
     "end_time": "2023-07-12T20:18:05.502991",
     "exception": false,
     "start_time": "2023-07-12T20:18:05.490885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_batch(batch):\n",
    "    for img_path in batch:\n",
    "        img_rgb = Image.open(img_path)\n",
    "        img_rgb.save(f'{get_name_from_path(img_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dcd117f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-12T20:18:05.513992Z",
     "iopub.status.busy": "2023-07-12T20:18:05.513233Z",
     "iopub.status.idle": "2023-07-12T20:18:05.517820Z",
     "shell.execute_reply": "2023-07-12T20:18:05.516981Z"
    },
    "papermill": {
     "duration": 0.012145,
     "end_time": "2023-07-12T20:18:05.519804",
     "exception": false,
     "start_time": "2023-07-12T20:18:05.507659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_grayscale_batch(batch):\n",
    "    return [f'{get_name_from_path(img_path)}' for img_path in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1411d9d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-12T20:18:05.530732Z",
     "iopub.status.busy": "2023-07-12T20:18:05.529911Z",
     "iopub.status.idle": "2023-07-12T20:18:05.534663Z",
     "shell.execute_reply": "2023-07-12T20:18:05.533826Z"
    },
    "papermill": {
     "duration": 0.012099,
     "end_time": "2023-07-12T20:18:05.536567",
     "exception": false,
     "start_time": "2023-07-12T20:18:05.524468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def del_grayscale_batch(batch):\n",
    "    for img_path in batch:\n",
    "        os.remove(f'{get_name_from_path(img_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a519f343",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-12T20:18:05.547216Z",
     "iopub.status.busy": "2023-07-12T20:18:05.546944Z",
     "iopub.status.idle": "2023-07-12T20:18:05.665957Z",
     "shell.execute_reply": "2023-07-12T20:18:05.665034Z"
    },
    "papermill": {
     "duration": 0.127012,
     "end_time": "2023-07-12T20:18:05.668269",
     "exception": false,
     "start_time": "2023-07-12T20:18:05.541257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class BaseColor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseColor, self).__init__()\n",
    "\n",
    "        self.l_cent = 50.\n",
    "        self.l_norm = 100.\n",
    "        self.ab_norm = 110.\n",
    "\n",
    "    def normalize_l(self, in_l):\n",
    "        return (in_l-self.l_cent)/self.l_norm\n",
    "\n",
    "    def unnormalize_l(self, in_l):\n",
    "        return in_l*self.l_norm + self.l_cent\n",
    "\n",
    "    def normalize_ab(self, in_ab):\n",
    "        return in_ab/self.ab_norm\n",
    "\n",
    "    def unnormalize_ab(self, in_ab):\n",
    "        return in_ab*self.ab_norm\n",
    "\n",
    "\n",
    "\n",
    "class ECCVGenerator(BaseColor):\n",
    "    def __init__(self, norm_layer=nn.BatchNorm2d):\n",
    "        super(ECCVGenerator, self).__init__()\n",
    "\n",
    "        model1=[nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model1+=[nn.ReLU(True),]\n",
    "        model1+=[nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=True),]\n",
    "        model1+=[nn.ReLU(True),]\n",
    "        model1+=[norm_layer(64),]\n",
    "\n",
    "        model2=[nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model2+=[nn.ReLU(True),]\n",
    "        model2+=[nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1, bias=True),]\n",
    "        model2+=[nn.ReLU(True),]\n",
    "        model2+=[norm_layer(128),]\n",
    "\n",
    "        model3=[nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model3+=[nn.ReLU(True),]\n",
    "        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model3+=[nn.ReLU(True),]\n",
    "        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1, bias=True),]\n",
    "        model3+=[nn.ReLU(True),]\n",
    "        model3+=[norm_layer(256),]\n",
    "\n",
    "        model4=[nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model4+=[nn.ReLU(True),]\n",
    "        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model4+=[nn.ReLU(True),]\n",
    "        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model4+=[nn.ReLU(True),]\n",
    "        model4+=[norm_layer(512),]\n",
    "\n",
    "        model5=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
    "        model5+=[nn.ReLU(True),]\n",
    "        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
    "        model5+=[nn.ReLU(True),]\n",
    "        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
    "        model5+=[nn.ReLU(True),]\n",
    "        model5+=[norm_layer(512),]\n",
    "\n",
    "        model6=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
    "        model6+=[nn.ReLU(True),]\n",
    "        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
    "        model6+=[nn.ReLU(True),]\n",
    "        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
    "        model6+=[nn.ReLU(True),]\n",
    "        model6+=[norm_layer(512),]\n",
    "\n",
    "        model7=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model7+=[nn.ReLU(True),]\n",
    "        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model7+=[nn.ReLU(True),]\n",
    "        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model7+=[nn.ReLU(True),]\n",
    "        model7+=[norm_layer(512),]\n",
    "\n",
    "        model8=[nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=True),]\n",
    "        model8+=[nn.ReLU(True),]\n",
    "        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model8+=[nn.ReLU(True),]\n",
    "        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model8+=[nn.ReLU(True),]\n",
    "\n",
    "        model8+=[nn.Conv2d(256, 313, kernel_size=1, stride=1, padding=0, bias=True),]\n",
    "\n",
    "        self.model1 = nn.Sequential(*model1)\n",
    "        self.model2 = nn.Sequential(*model2)\n",
    "        self.model3 = nn.Sequential(*model3)\n",
    "        self.model4 = nn.Sequential(*model4)\n",
    "        self.model5 = nn.Sequential(*model5)\n",
    "        self.model6 = nn.Sequential(*model6)\n",
    "        self.model7 = nn.Sequential(*model7)\n",
    "        self.model8 = nn.Sequential(*model8)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.model_out = nn.Conv2d(313, 2, kernel_size=1, padding=0, dilation=1, stride=1, bias=False)\n",
    "        self.upsample4 = nn.Upsample(scale_factor=4, mode='bilinear')\n",
    "\n",
    "    def forward(self, input_l):\n",
    "        conv1_2 = self.model1(self.normalize_l(input_l))\n",
    "        conv2_2 = self.model2(conv1_2)\n",
    "        conv3_3 = self.model3(conv2_2)\n",
    "        conv4_3 = self.model4(conv3_3)\n",
    "        conv5_3 = self.model5(conv4_3)\n",
    "        conv6_3 = self.model6(conv5_3)\n",
    "        conv7_3 = self.model7(conv6_3)\n",
    "        conv8_3 = self.model8(conv7_3)\n",
    "        out_reg = self.model_out(self.softmax(conv8_3))\n",
    "\n",
    "        return self.unnormalize_ab(self.upsample4(out_reg))\n",
    "\n",
    "def eccv16(pretrained=True):\n",
    "    model = ECCVGenerator()\n",
    "    if(pretrained):\n",
    "        import torch.utils.model_zoo as model_zoo\n",
    "        model.load_state_dict(model_zoo.load_url('https://colorizers.s3.us-east-2.amazonaws.com/colorization_release_v2-9b330a0b.pth',map_location='cpu',check_hash=True))\n",
    "    return model\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage import color\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# def load_img(img_path):\n",
    "def load_imgs(img_path_list):\n",
    "    out_np_list = [np.asarray(Image.open(img_path).convert('RGB'))\n",
    "                   for img_path in img_path_list]\n",
    "    \n",
    "    # if(out_np.ndim==2):\n",
    "    #    out_np = np.tile(out_np[:,:,None],3)\n",
    "    for i, out_np in enumerate(out_np_list):\n",
    "        if out_np.ndim==2:\n",
    "            out_np_list[i] = np.tile(out_np[:,:,None],3)\n",
    "    # return out_np\n",
    "    return out_np_list\n",
    "\n",
    "def resize_img(img, HW=(256,256), resample=3):\n",
    "    return np.asarray(Image.fromarray(img).resize((HW[1],HW[0]), resample=resample))\n",
    "\n",
    "# def preprocess_img(img_rgb_orig, HW=(256,256), resample=3):\n",
    "def preprocess_imgs(img_rgb_orig_list, HW=(256,256), resample=3):\n",
    "    # return original size L and resized L as torch Tensors\n",
    "    # img_rgb_rs = resize_img(img_rgb_orig, HW=HW, resample=resample)\n",
    "    img_rgb_rs_list = [resize_img(img_rgb_orig, HW=HW, resample=resample)\n",
    "                       for img_rgb_orig in img_rgb_orig_list]\n",
    "\n",
    "    # img_lab_orig = color.rgb2lab(img_rgb_orig)\n",
    "    # img_lab_rs = color.rgb2lab(img_rgb_rs)\n",
    "    img_lab_orig_list = [color.rgb2lab(img_rgb_orig)\n",
    "                         for img_rgb_orig in img_rgb_orig_list]\n",
    "    img_lab_rs_list = [color.rgb2lab(img_rgb_rs)\n",
    "                       for img_rgb_rs in img_rgb_rs_list]\n",
    "\n",
    "    # img_l_orig = img_lab_orig[:,:,0]\n",
    "    # img_l_rs = img_lab_rs[:,:,0]\n",
    "    img_l_orig_list = [img_lab_orig[:,:,0]\n",
    "                       for img_lab_orig in img_lab_orig_list]\n",
    "    img_l_rs_list = [img_lab_rs[:,:,0]\n",
    "                     for img_lab_rs in img_lab_rs_list]\n",
    "    \n",
    "    # FOR GT\n",
    "    # (256, 256, 2) 1 2 -> 0 1\n",
    "    img_ab_rs_list = [torch.transpose(torch.transpose(torch.Tensor(img_lab_rs[:,:,1:3]), 1, 2), 0, 1)\n",
    "                     for img_lab_rs in img_lab_rs_list]\n",
    "\n",
    "    # RESHAPE FIRST\n",
    "    img_l_rs_list_reshaped = [torch.Tensor(img_l_rs)[None, None, :, :]\n",
    "                              for img_l_rs in img_l_rs_list]\n",
    "    img_ab_rs_list_reshaped = [torch.Tensor(img_ab_rs)[None,:, :, :]\n",
    "                              for img_ab_rs in img_ab_rs_list]\n",
    "\n",
    "    # tens_orig_l = torch.Tensor(img_l_orig)[None,None,:,:]\n",
    "    # tens_rs_l = torch.Tensor(img_l_rs)[None,None,:,:]\n",
    "    tens_orig_l_list = [torch.Tensor(img_l_orig)[None,None,:,:]\n",
    "                        for img_l_orig in img_l_orig_list]\n",
    "    \n",
    "    batch_size = len(img_rgb_orig_list)\n",
    "    tens_rs_l_tensor = torch.cat(img_l_rs_list_reshaped, dim=0)\n",
    "    tens_rs_ab_tensor = torch.cat(img_ab_rs_list_reshaped, dim=0)\n",
    "\n",
    "    return (tens_orig_l_list, tens_rs_l_tensor, tens_rs_ab_tensor)\n",
    "\n",
    "# def postprocess_tens(tens_orig_l, out_ab, mode='bilinear'):\n",
    "def postprocess_imgs(tens_orig_l_list, out_ab, mode='bilinear'):\n",
    "    # tens_orig_l     1 x 1 x H_orig x W_orig\n",
    "    # out_ab         1 x 2 x H x W\n",
    "    \n",
    "    # tens_orig_l_list: (16 elements) 1 x 1 x H_orig x W_orig\n",
    "    # out_ab                          16 x 2 x H x W\n",
    "\n",
    "    # HW_orig = tens_orig_l.shape[2:]\n",
    "    # HW = out_ab.shape[2:]\n",
    "    HW_orig_list = [tens_orig_l.shape[2:] for tens_orig_l in tens_orig_l_list]  # 16 ele H_orig x W_orig\n",
    "    HW = out_ab.shape[2:]  # (H, W)\n",
    "\n",
    "    # call resize function if needed\n",
    "    #if(HW_orig[0]!=HW[0] or HW_orig[1]!=HW[1]):\n",
    "    #    out_ab_orig = F.interpolate(out_ab, size=HW_orig, mode='bilinear')\n",
    "    #else:\n",
    "    #    out_ab_orig = out_ab\n",
    "    \n",
    "    out_ab_orig_list = list()\n",
    "    for i, HW_orig in enumerate(HW_orig_list):\n",
    "        if(HW_orig[0]!=HW[0] or HW_orig[1]!=HW[1]):\n",
    "            out_ab_orig = F.interpolate(out_ab[i][None, :, :, :], size=HW_orig, mode='bilinear')\n",
    "        else:\n",
    "            out_ab_orig = out_ab[i][None, :, :, :]     \n",
    "        out_ab_orig_list.append(out_ab_orig)\n",
    "\n",
    "    # out_lab_orig = torch.cat((tens_orig_l, out_ab_orig), dim=1)\n",
    "    out_lab_orig_list = list()\n",
    "    for i in range(len(out_ab_orig_list)):\n",
    "        out_lab_orig_list.append(\n",
    "            torch.cat((tens_orig_l_list[i].cuda(), out_ab_orig_list[i]), dim=1)\n",
    "        )\n",
    "    \n",
    "    # return color.lab2rgb(out_lab_orig.data.cpu().numpy()[0,...].transpose((1,2,0)))\n",
    "    return [color.lab2rgb(out_lab_orig.data.cpu().numpy()[0,...].transpose((1,2,0)))\n",
    "           for out_lab_orig in out_lab_orig_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3c871f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-12T20:18:05.682528Z",
     "iopub.status.busy": "2023-07-12T20:18:05.682249Z",
     "iopub.status.idle": "2023-07-12T20:18:14.026722Z",
     "shell.execute_reply": "2023-07-12T20:18:14.025736Z"
    },
    "papermill": {
     "duration": 8.352571,
     "end_time": "2023-07-12T20:18:14.029051",
     "exception": false,
     "start_time": "2023-07-12T20:18:05.676480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://colorizers.s3.us-east-2.amazonaws.com/colorization_release_v2-9b330a0b.pth\" to /root/.cache/torch/hub/checkpoints/colorization_release_v2-9b330a0b.pth\n",
      "100%|██████████| 123M/123M [00:04<00:00, 29.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = eccv16(pretrained=True).cuda()\n",
    "model.train()\n",
    "# model.load_state_dict(torch.load('/kaggle/input/eccv16-train-epoch-14/model_14.pth'))\n",
    "optimizer = torch.optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c52d73",
   "metadata": {
    "papermill": {
     "duration": 0.007292,
     "end_time": "2023-07-12T20:18:14.044084",
     "exception": false,
     "start_time": "2023-07-12T20:18:14.036792",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "(689, 173) batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f35532f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-12T20:18:14.059972Z",
     "iopub.status.busy": "2023-07-12T20:18:14.059686Z",
     "iopub.status.idle": "2023-07-13T07:09:07.707934Z",
     "shell.execute_reply": "2023-07-13T07:09:07.706661Z"
    },
    "papermill": {
     "duration": 39053.661728,
     "end_time": "2023-07-13T07:09:07.713153",
     "exception": false,
     "start_time": "2023-07-12T20:18:14.051425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Batch 0: loss=230.8716583251953\n",
      "Batch 10: loss=84.85639953613281\n",
      "Batch 20: loss=93.0570297241211\n",
      "Batch 30: loss=105.48080444335938\n",
      "Batch 40: loss=110.83549499511719\n",
      "Batch 50: loss=81.59423828125\n",
      "Batch 60: loss=133.99737548828125\n",
      "Batch 70: loss=93.16462707519531\n",
      "Batch 80: loss=147.85980224609375\n",
      "Batch 90: loss=108.5198745727539\n",
      "Batch 100: loss=90.40885162353516\n",
      "Batch 110: loss=75.20344543457031\n",
      "Batch 120: loss=86.13192749023438\n",
      "Batch 130: loss=74.51362609863281\n",
      "Batch 140: loss=100.82162475585938\n",
      "Batch 150: loss=168.13232421875\n",
      "Batch 160: loss=66.36837768554688\n",
      "Batch 170: loss=124.9740982055664\n",
      "Batch 180: loss=70.95185089111328\n",
      "Batch 190: loss=163.56787109375\n",
      "Batch 200: loss=137.35235595703125\n",
      "Batch 210: loss=108.93681335449219\n",
      "Batch 220: loss=117.53683471679688\n",
      "Batch 230: loss=95.5561294555664\n",
      "Batch 240: loss=131.8926544189453\n",
      "Batch 250: loss=53.789710998535156\n",
      "Batch 260: loss=80.94195556640625\n",
      "Batch 270: loss=91.37062072753906\n",
      "Batch 280: loss=94.54707336425781\n",
      "Batch 290: loss=92.21084594726562\n",
      "Batch 300: loss=154.8809814453125\n",
      "Batch 310: loss=81.13511657714844\n",
      "Batch 320: loss=100.01808166503906\n",
      "Batch 330: loss=150.81788635253906\n",
      "Batch 340: loss=62.93581771850586\n",
      "Batch 350: loss=114.1862564086914\n",
      "Batch 360: loss=137.06463623046875\n",
      "Batch 370: loss=90.53286743164062\n",
      "Batch 380: loss=67.08009338378906\n",
      "Batch 390: loss=108.96255493164062\n",
      "Batch 400: loss=83.29696655273438\n",
      "Batch 410: loss=122.84814453125\n",
      "Batch 420: loss=80.73699951171875\n",
      "Batch 430: loss=79.83183288574219\n",
      "Batch 440: loss=95.44844818115234\n",
      "Batch 450: loss=169.93869018554688\n",
      "Batch 460: loss=94.26673889160156\n",
      "Batch 470: loss=179.80767822265625\n",
      "Batch 480: loss=109.48866271972656\n",
      "Batch 490: loss=106.14488983154297\n",
      "Batch 500: loss=76.0063705444336\n",
      "Batch 510: loss=84.60345458984375\n",
      "Batch 520: loss=102.53840637207031\n",
      "Batch 530: loss=147.542724609375\n",
      "Batch 540: loss=117.42575073242188\n",
      "Batch 550: loss=102.09092712402344\n",
      "Batch 560: loss=79.50425720214844\n",
      "Batch 570: loss=121.91419982910156\n",
      "Batch 580: loss=100.34984588623047\n",
      "Batch 590: loss=79.42298889160156\n",
      "Batch 600: loss=90.93511962890625\n",
      "Batch 610: loss=187.83261108398438\n",
      "Batch 620: loss=88.41107177734375\n",
      "Batch 630: loss=76.5589599609375\n",
      "Batch 640: loss=98.4198989868164\n",
      "Batch 650: loss=115.28961181640625\n",
      "Batch 660: loss=96.57319641113281\n",
      "Batch 670: loss=124.55745697021484\n",
      "Batch 680: loss=57.83625793457031\n",
      "Epoch 1\n",
      "Batch 0: loss=97.36061096191406\n",
      "Batch 10: loss=85.68522644042969\n",
      "Batch 20: loss=93.11457061767578\n",
      "Batch 30: loss=105.48523712158203\n",
      "Batch 40: loss=110.80256652832031\n",
      "Batch 50: loss=81.75001525878906\n",
      "Batch 60: loss=134.05258178710938\n",
      "Batch 70: loss=93.17352294921875\n",
      "Batch 80: loss=147.86376953125\n",
      "Batch 90: loss=108.50438690185547\n",
      "Batch 100: loss=90.41100311279297\n",
      "Batch 110: loss=75.2000961303711\n",
      "Batch 120: loss=86.13636779785156\n",
      "Batch 130: loss=74.50608825683594\n",
      "Batch 140: loss=100.81854248046875\n",
      "Batch 150: loss=168.13250732421875\n",
      "Batch 160: loss=66.37661743164062\n",
      "Batch 170: loss=124.97518920898438\n",
      "Batch 180: loss=70.95149993896484\n",
      "Batch 190: loss=163.56942749023438\n",
      "Batch 200: loss=137.3444061279297\n",
      "Batch 210: loss=108.94148254394531\n",
      "Batch 220: loss=117.5374755859375\n",
      "Batch 230: loss=95.56082153320312\n",
      "Batch 240: loss=131.8916473388672\n",
      "Batch 250: loss=53.7896728515625\n",
      "Batch 260: loss=80.93968200683594\n",
      "Batch 270: loss=91.37480926513672\n",
      "Batch 280: loss=94.54737854003906\n",
      "Batch 290: loss=92.21165466308594\n",
      "Batch 300: loss=154.8826141357422\n",
      "Batch 310: loss=81.13253784179688\n",
      "Batch 320: loss=100.01739501953125\n",
      "Batch 330: loss=150.812255859375\n",
      "Batch 340: loss=62.934295654296875\n",
      "Batch 350: loss=114.18663024902344\n",
      "Batch 360: loss=137.06402587890625\n",
      "Batch 370: loss=90.52593994140625\n",
      "Batch 380: loss=67.07099914550781\n",
      "Batch 390: loss=108.96002960205078\n",
      "Batch 400: loss=83.29591369628906\n",
      "Batch 410: loss=122.89585876464844\n",
      "Batch 420: loss=80.73176574707031\n",
      "Batch 430: loss=79.84359741210938\n",
      "Batch 440: loss=95.45308685302734\n",
      "Batch 450: loss=169.92929077148438\n",
      "Batch 460: loss=94.26704406738281\n",
      "Batch 470: loss=179.8070068359375\n",
      "Batch 480: loss=109.48847961425781\n",
      "Batch 490: loss=106.14454650878906\n",
      "Batch 500: loss=76.0069580078125\n",
      "Batch 510: loss=84.60670471191406\n",
      "Batch 520: loss=102.53451538085938\n",
      "Batch 530: loss=147.5421600341797\n",
      "Batch 540: loss=117.4289321899414\n",
      "Batch 550: loss=102.08760070800781\n",
      "Batch 560: loss=79.50505065917969\n",
      "Batch 570: loss=121.91534423828125\n",
      "Batch 580: loss=100.353759765625\n",
      "Batch 590: loss=79.42130279541016\n",
      "Batch 600: loss=90.93950653076172\n",
      "Batch 610: loss=187.828125\n",
      "Batch 620: loss=88.41179656982422\n",
      "Batch 630: loss=76.55402374267578\n",
      "Batch 640: loss=98.41618347167969\n",
      "Batch 650: loss=115.28972625732422\n",
      "Batch 660: loss=96.57278442382812\n",
      "Batch 670: loss=124.55606079101562\n",
      "Batch 680: loss=57.83653259277344\n",
      "Epoch 2\n",
      "Batch 0: loss=97.35995483398438\n",
      "Batch 10: loss=85.68522644042969\n",
      "Batch 20: loss=93.11474609375\n",
      "Batch 30: loss=105.48542785644531\n",
      "Batch 40: loss=110.80299377441406\n",
      "Batch 50: loss=81.75141906738281\n",
      "Batch 60: loss=134.0531768798828\n",
      "Batch 70: loss=93.173828125\n",
      "Batch 80: loss=147.8638458251953\n",
      "Batch 90: loss=108.50477600097656\n",
      "Batch 100: loss=90.41071319580078\n",
      "Batch 110: loss=75.20014190673828\n",
      "Batch 120: loss=86.13665771484375\n",
      "Batch 130: loss=74.50550842285156\n",
      "Batch 140: loss=100.81825256347656\n",
      "Batch 150: loss=168.13250732421875\n",
      "Batch 160: loss=66.37738037109375\n",
      "Batch 170: loss=124.97528076171875\n",
      "Batch 180: loss=70.95144653320312\n",
      "Batch 190: loss=163.56961059570312\n",
      "Batch 200: loss=137.343505859375\n",
      "Batch 210: loss=108.9420394897461\n",
      "Batch 220: loss=117.53755950927734\n",
      "Batch 230: loss=95.5614013671875\n",
      "Batch 240: loss=131.89157104492188\n",
      "Batch 250: loss=53.789669036865234\n",
      "Batch 260: loss=80.93936157226562\n",
      "Batch 270: loss=91.37544250488281\n",
      "Batch 280: loss=94.54742431640625\n",
      "Batch 290: loss=92.21177673339844\n",
      "Batch 300: loss=154.88284301757812\n",
      "Batch 310: loss=81.13212585449219\n",
      "Batch 320: loss=100.01728820800781\n",
      "Batch 330: loss=150.8113250732422\n",
      "Batch 340: loss=62.93403625488281\n",
      "Batch 350: loss=114.18669128417969\n",
      "Batch 360: loss=137.06393432617188\n",
      "Batch 370: loss=90.5246810913086\n",
      "Batch 380: loss=67.0693359375\n",
      "Batch 390: loss=108.95954895019531\n",
      "Batch 400: loss=83.29570007324219\n",
      "Batch 410: loss=122.90518188476562\n",
      "Batch 420: loss=80.73075866699219\n",
      "Batch 430: loss=79.84602355957031\n",
      "Batch 440: loss=95.45404052734375\n",
      "Batch 450: loss=169.92739868164062\n",
      "Batch 460: loss=94.2671127319336\n",
      "Batch 470: loss=179.80686950683594\n",
      "Batch 480: loss=109.48843383789062\n",
      "Batch 490: loss=106.14447021484375\n",
      "Batch 500: loss=76.00709533691406\n",
      "Batch 510: loss=84.6074447631836\n",
      "Batch 520: loss=102.53360748291016\n",
      "Batch 530: loss=147.54202270507812\n",
      "Batch 540: loss=117.42969512939453\n",
      "Batch 550: loss=102.0867919921875\n",
      "Batch 560: loss=79.50524139404297\n",
      "Batch 570: loss=121.91563415527344\n",
      "Batch 580: loss=100.35475158691406\n",
      "Batch 590: loss=79.42086791992188\n",
      "Batch 600: loss=90.9406509399414\n",
      "Batch 610: loss=187.82696533203125\n",
      "Batch 620: loss=88.41197204589844\n",
      "Batch 630: loss=76.55268859863281\n",
      "Batch 640: loss=98.41519165039062\n",
      "Batch 650: loss=115.28976440429688\n",
      "Batch 660: loss=96.57267761230469\n",
      "Batch 670: loss=124.55567932128906\n",
      "Batch 680: loss=57.83661651611328\n",
      "Epoch 3\n",
      "Batch 0: loss=97.35975646972656\n",
      "Batch 10: loss=85.68523406982422\n",
      "Batch 20: loss=93.11479949951172\n",
      "Batch 30: loss=105.4854736328125\n",
      "Batch 40: loss=110.8031234741211\n",
      "Batch 50: loss=81.7518310546875\n",
      "Batch 60: loss=134.0533447265625\n",
      "Batch 70: loss=93.17391204833984\n",
      "Batch 80: loss=147.8638916015625\n",
      "Batch 90: loss=108.5048828125\n",
      "Batch 100: loss=90.4106216430664\n",
      "Batch 110: loss=75.20014953613281\n",
      "Batch 120: loss=86.13674926757812\n",
      "Batch 130: loss=74.50532531738281\n",
      "Batch 140: loss=100.81816101074219\n",
      "Batch 150: loss=168.13250732421875\n",
      "Batch 160: loss=66.37762451171875\n",
      "Batch 170: loss=124.97531127929688\n",
      "Batch 180: loss=70.95142364501953\n",
      "Batch 190: loss=163.56967163085938\n",
      "Batch 200: loss=137.34320068359375\n",
      "Batch 210: loss=108.94223022460938\n",
      "Batch 220: loss=117.5375747680664\n",
      "Batch 230: loss=95.56159210205078\n",
      "Batch 240: loss=131.8915557861328\n",
      "Batch 250: loss=53.789669036865234\n",
      "Batch 260: loss=80.93924713134766\n",
      "Batch 270: loss=91.37565612792969\n",
      "Batch 280: loss=94.54743957519531\n",
      "Batch 290: loss=92.21182250976562\n",
      "Batch 300: loss=154.8829345703125\n",
      "Batch 310: loss=81.13199615478516\n",
      "Batch 320: loss=100.01724243164062\n",
      "Batch 330: loss=150.81100463867188\n",
      "Batch 340: loss=62.933937072753906\n",
      "Batch 350: loss=114.18672180175781\n",
      "Batch 360: loss=137.06390380859375\n",
      "Batch 370: loss=90.52423095703125\n",
      "Batch 380: loss=67.06874084472656\n",
      "Batch 390: loss=108.95938110351562\n",
      "Batch 400: loss=83.29562377929688\n",
      "Batch 410: loss=122.9085693359375\n",
      "Batch 420: loss=80.73038482666016\n",
      "Batch 430: loss=79.84690856933594\n",
      "Batch 440: loss=95.45438385009766\n",
      "Batch 450: loss=169.92669677734375\n",
      "Batch 460: loss=94.26713562011719\n",
      "Batch 470: loss=179.80682373046875\n",
      "Batch 480: loss=109.48841857910156\n",
      "Batch 490: loss=106.14445495605469\n",
      "Batch 500: loss=76.00715637207031\n",
      "Batch 510: loss=84.60772705078125\n",
      "Batch 520: loss=102.53326416015625\n",
      "Batch 530: loss=147.54196166992188\n",
      "Batch 540: loss=117.42998504638672\n",
      "Batch 550: loss=102.08647918701172\n",
      "Batch 560: loss=79.50532531738281\n",
      "Batch 570: loss=121.91575622558594\n",
      "Batch 580: loss=100.35514831542969\n",
      "Batch 590: loss=79.42070007324219\n",
      "Batch 600: loss=90.94109344482422\n",
      "Batch 610: loss=187.82650756835938\n",
      "Batch 620: loss=88.41204071044922\n",
      "Batch 630: loss=76.55215454101562\n",
      "Batch 640: loss=98.41480255126953\n",
      "Batch 650: loss=115.28977966308594\n",
      "Batch 660: loss=96.57262420654297\n",
      "Batch 670: loss=124.5555191040039\n",
      "Batch 680: loss=57.836647033691406\n",
      "Epoch 4\n",
      "Batch 0: loss=97.35968780517578\n",
      "Batch 10: loss=85.68522644042969\n",
      "Batch 20: loss=93.11481475830078\n",
      "Batch 30: loss=105.4854965209961\n",
      "Batch 40: loss=110.80316925048828\n",
      "Batch 50: loss=81.75199890136719\n",
      "Batch 60: loss=134.05340576171875\n",
      "Batch 70: loss=93.17394256591797\n",
      "Batch 80: loss=147.86387634277344\n",
      "Batch 90: loss=108.50492858886719\n",
      "Batch 100: loss=90.41058349609375\n",
      "Batch 110: loss=75.20015716552734\n",
      "Batch 120: loss=86.13677978515625\n",
      "Batch 130: loss=74.5052490234375\n",
      "Batch 140: loss=100.81812286376953\n",
      "Batch 150: loss=168.13250732421875\n",
      "Batch 160: loss=66.37773132324219\n",
      "Batch 170: loss=124.97533416748047\n",
      "Batch 180: loss=70.951416015625\n",
      "Batch 190: loss=163.56968688964844\n",
      "Batch 200: loss=137.3430633544922\n",
      "Batch 210: loss=108.94229888916016\n",
      "Batch 220: loss=117.53758239746094\n",
      "Batch 230: loss=95.56167602539062\n",
      "Batch 240: loss=131.8915557861328\n",
      "Batch 250: loss=53.78966522216797\n",
      "Batch 260: loss=80.93920135498047\n",
      "Batch 270: loss=91.37574768066406\n",
      "Batch 280: loss=94.54745483398438\n",
      "Batch 290: loss=92.21183776855469\n",
      "Batch 300: loss=154.88296508789062\n",
      "Batch 310: loss=81.13192749023438\n",
      "Batch 320: loss=100.01722717285156\n",
      "Batch 330: loss=150.81085205078125\n",
      "Batch 340: loss=62.933895111083984\n",
      "Batch 350: loss=114.18672943115234\n",
      "Batch 360: loss=137.06390380859375\n",
      "Batch 370: loss=90.52403259277344\n",
      "Batch 380: loss=67.0684814453125\n",
      "Batch 390: loss=108.95930480957031\n",
      "Batch 400: loss=83.29559326171875\n",
      "Batch 410: loss=122.91004180908203\n",
      "Batch 420: loss=80.730224609375\n",
      "Batch 430: loss=79.84729766845703\n",
      "Batch 440: loss=95.45452880859375\n",
      "Batch 450: loss=169.9263916015625\n",
      "Batch 460: loss=94.26715087890625\n",
      "Batch 470: loss=179.80677795410156\n",
      "Batch 480: loss=109.48841094970703\n",
      "Batch 490: loss=106.1444320678711\n",
      "Batch 500: loss=76.0071792602539\n",
      "Batch 510: loss=84.60784912109375\n",
      "Batch 520: loss=102.5331039428711\n",
      "Batch 530: loss=147.5419464111328\n",
      "Batch 540: loss=117.43011474609375\n",
      "Batch 550: loss=102.08633422851562\n",
      "Batch 560: loss=79.50535583496094\n",
      "Batch 570: loss=121.91580963134766\n",
      "Batch 580: loss=100.35530090332031\n",
      "Batch 590: loss=79.42062377929688\n",
      "Batch 600: loss=90.94129180908203\n",
      "Batch 610: loss=187.8262939453125\n",
      "Batch 620: loss=88.41206359863281\n",
      "Batch 630: loss=76.55192565917969\n",
      "Batch 640: loss=98.41462707519531\n",
      "Batch 650: loss=115.289794921875\n",
      "Batch 660: loss=96.57260131835938\n",
      "Batch 670: loss=124.55545043945312\n",
      "Batch 680: loss=57.83666229248047\n",
      "Epoch 5\n",
      "Batch 0: loss=97.35964965820312\n",
      "Batch 10: loss=85.68523406982422\n",
      "Batch 20: loss=93.11483001708984\n",
      "Batch 30: loss=105.48550415039062\n",
      "Batch 40: loss=110.80319213867188\n",
      "Batch 50: loss=81.75208282470703\n",
      "Batch 60: loss=134.05343627929688\n",
      "Batch 70: loss=93.17395782470703\n",
      "Batch 80: loss=147.8638916015625\n",
      "Batch 90: loss=108.50495910644531\n",
      "Batch 100: loss=90.41056823730469\n",
      "Batch 110: loss=75.20016479492188\n",
      "Batch 120: loss=86.13680267333984\n",
      "Batch 130: loss=74.50521850585938\n",
      "Batch 140: loss=100.818115234375\n",
      "Batch 150: loss=168.13250732421875\n",
      "Batch 160: loss=66.37777709960938\n",
      "Batch 170: loss=124.97533416748047\n",
      "Batch 180: loss=70.951416015625\n",
      "Batch 190: loss=163.5697021484375\n",
      "Batch 200: loss=137.343017578125\n",
      "Batch 210: loss=108.94233703613281\n",
      "Batch 220: loss=117.53759765625\n",
      "Batch 230: loss=95.56170654296875\n",
      "Batch 240: loss=131.8915557861328\n",
      "Batch 250: loss=53.78966522216797\n",
      "Batch 260: loss=80.93917846679688\n",
      "Batch 270: loss=91.37579345703125\n",
      "Batch 280: loss=94.54745483398438\n",
      "Batch 290: loss=92.21185302734375\n",
      "Batch 300: loss=154.88299560546875\n",
      "Batch 310: loss=81.13189697265625\n",
      "Batch 320: loss=100.0172119140625\n",
      "Batch 330: loss=150.810791015625\n",
      "Batch 340: loss=62.93387222290039\n",
      "Batch 350: loss=114.18673706054688\n",
      "Batch 360: loss=137.0638885498047\n",
      "Batch 370: loss=90.5239486694336\n",
      "Batch 380: loss=67.068359375\n",
      "Batch 390: loss=108.95927429199219\n",
      "Batch 400: loss=83.29557800292969\n",
      "Batch 410: loss=122.91072845458984\n",
      "Batch 420: loss=80.73014831542969\n",
      "Batch 430: loss=79.84748840332031\n",
      "Batch 440: loss=95.45460510253906\n",
      "Batch 450: loss=169.92623901367188\n",
      "Batch 460: loss=94.26715087890625\n",
      "Batch 470: loss=179.80679321289062\n",
      "Batch 480: loss=109.4884033203125\n",
      "Batch 490: loss=106.1444320678711\n",
      "Batch 500: loss=76.00718688964844\n",
      "Batch 510: loss=84.60791015625\n",
      "Batch 520: loss=102.53303527832031\n",
      "Batch 530: loss=147.54193115234375\n",
      "Batch 540: loss=117.43019104003906\n",
      "Batch 550: loss=102.08627319335938\n",
      "Batch 560: loss=79.50536346435547\n",
      "Batch 570: loss=121.91582489013672\n",
      "Batch 580: loss=100.35539245605469\n",
      "Batch 590: loss=79.42057800292969\n",
      "Batch 600: loss=90.94139099121094\n",
      "Batch 610: loss=187.82620239257812\n",
      "Batch 620: loss=88.41207885742188\n",
      "Batch 630: loss=76.55181884765625\n",
      "Batch 640: loss=98.41454315185547\n",
      "Batch 650: loss=115.28980255126953\n",
      "Batch 660: loss=96.57259368896484\n",
      "Batch 670: loss=124.555419921875\n",
      "Batch 680: loss=57.836669921875\n",
      "Epoch 6\n",
      "Batch 0: loss=97.35963439941406\n",
      "Batch 10: loss=85.68522644042969\n",
      "Batch 20: loss=93.11483764648438\n",
      "Batch 30: loss=105.48550415039062\n",
      "Batch 40: loss=110.8031997680664\n",
      "Batch 50: loss=81.75212097167969\n",
      "Batch 60: loss=134.053466796875\n",
      "Batch 70: loss=93.17396545410156\n",
      "Batch 80: loss=147.8638916015625\n",
      "Batch 90: loss=108.50495910644531\n",
      "Batch 100: loss=90.41056823730469\n",
      "Batch 110: loss=75.20014953613281\n",
      "Batch 120: loss=86.13681030273438\n",
      "Batch 130: loss=74.50520324707031\n",
      "Batch 140: loss=100.81809997558594\n",
      "Batch 150: loss=168.13250732421875\n",
      "Batch 160: loss=66.37779998779297\n",
      "Batch 170: loss=124.975341796875\n",
      "Batch 180: loss=70.951416015625\n",
      "Batch 190: loss=163.5697021484375\n",
      "Batch 200: loss=137.34298706054688\n",
      "Batch 210: loss=108.94235229492188\n",
      "Batch 220: loss=117.53759765625\n",
      "Batch 230: loss=95.56172180175781\n",
      "Batch 240: loss=131.89154052734375\n",
      "Batch 250: loss=53.78966522216797\n",
      "Batch 260: loss=80.93917083740234\n",
      "Batch 270: loss=91.37580871582031\n",
      "Batch 280: loss=94.54745483398438\n",
      "Batch 290: loss=92.21185302734375\n",
      "Batch 300: loss=154.88299560546875\n",
      "Batch 310: loss=81.13188934326172\n",
      "Batch 320: loss=100.01721954345703\n",
      "Batch 330: loss=150.81076049804688\n",
      "Batch 340: loss=62.93386459350586\n",
      "Batch 350: loss=114.1867446899414\n",
      "Batch 360: loss=137.06387329101562\n",
      "Batch 370: loss=90.52389526367188\n",
      "Batch 380: loss=67.06829833984375\n",
      "Batch 390: loss=108.95925903320312\n",
      "Batch 400: loss=83.29557800292969\n",
      "Batch 410: loss=122.91107177734375\n",
      "Batch 420: loss=80.73011016845703\n",
      "Batch 430: loss=79.84757232666016\n",
      "Batch 440: loss=95.45463562011719\n",
      "Batch 450: loss=169.92617797851562\n",
      "Batch 460: loss=94.26715850830078\n",
      "Batch 470: loss=179.80677795410156\n",
      "Batch 480: loss=109.4884033203125\n",
      "Batch 490: loss=106.14442443847656\n",
      "Batch 500: loss=76.00719451904297\n",
      "Batch 510: loss=84.60794067382812\n",
      "Batch 520: loss=102.53299713134766\n",
      "Batch 530: loss=147.54193115234375\n",
      "Batch 540: loss=117.43021392822266\n",
      "Batch 550: loss=102.08624267578125\n",
      "Batch 560: loss=79.50537109375\n",
      "Batch 570: loss=121.91584014892578\n",
      "Batch 580: loss=100.35543823242188\n",
      "Batch 590: loss=79.42056274414062\n",
      "Batch 600: loss=90.9414291381836\n",
      "Batch 610: loss=187.826171875\n",
      "Batch 620: loss=88.41209411621094\n",
      "Batch 630: loss=76.5517578125\n",
      "Batch 640: loss=98.41450500488281\n",
      "Batch 650: loss=115.289794921875\n",
      "Batch 660: loss=96.57258605957031\n",
      "Batch 670: loss=124.5553970336914\n",
      "Batch 680: loss=57.836669921875\n",
      "Epoch 7\n",
      "Batch 0: loss=97.35963439941406\n",
      "Batch 10: loss=85.68523406982422\n",
      "Batch 20: loss=93.11483764648438\n",
      "Batch 30: loss=105.48551177978516\n",
      "Batch 40: loss=110.80320739746094\n",
      "Batch 50: loss=81.75213623046875\n",
      "Batch 60: loss=134.053466796875\n",
      "Batch 70: loss=93.17396545410156\n",
      "Batch 80: loss=147.8638916015625\n",
      "Batch 90: loss=108.50497436523438\n",
      "Batch 100: loss=90.41056060791016\n",
      "Batch 110: loss=75.20015716552734\n",
      "Batch 120: loss=86.13681030273438\n",
      "Batch 130: loss=74.50520324707031\n",
      "Batch 140: loss=100.81809997558594\n",
      "Batch 150: loss=168.13250732421875\n",
      "Batch 160: loss=66.3778076171875\n",
      "Batch 170: loss=124.975341796875\n",
      "Batch 180: loss=70.951416015625\n",
      "Batch 190: loss=163.56971740722656\n",
      "Batch 200: loss=137.3429718017578\n",
      "Batch 210: loss=108.9423599243164\n",
      "Batch 220: loss=117.53759765625\n",
      "Batch 230: loss=95.56172943115234\n",
      "Batch 240: loss=131.8915557861328\n",
      "Batch 250: loss=53.78966522216797\n",
      "Batch 260: loss=80.93916320800781\n",
      "Batch 270: loss=91.37582397460938\n",
      "Batch 280: loss=94.54745483398438\n",
      "Batch 290: loss=92.21185302734375\n",
      "Batch 300: loss=154.8830108642578\n",
      "Batch 310: loss=81.13188171386719\n",
      "Batch 320: loss=100.01721954345703\n",
      "Batch 330: loss=150.8107452392578\n",
      "Batch 340: loss=62.933860778808594\n",
      "Batch 350: loss=114.1867446899414\n",
      "Batch 360: loss=137.06387329101562\n",
      "Batch 370: loss=90.52388000488281\n",
      "Batch 380: loss=67.06827545166016\n",
      "Batch 390: loss=108.95924377441406\n",
      "Batch 400: loss=83.29557037353516\n",
      "Batch 410: loss=122.91124725341797\n",
      "Batch 420: loss=80.73009490966797\n",
      "Batch 430: loss=79.84761047363281\n",
      "Batch 440: loss=95.45465850830078\n",
      "Batch 450: loss=169.9261474609375\n",
      "Batch 460: loss=94.26715850830078\n",
      "Batch 470: loss=179.8067626953125\n",
      "Batch 480: loss=109.4884033203125\n",
      "Batch 490: loss=106.14442443847656\n",
      "Batch 500: loss=76.0072021484375\n",
      "Batch 510: loss=84.60795593261719\n",
      "Batch 520: loss=102.53297424316406\n",
      "Batch 530: loss=147.5419158935547\n",
      "Batch 540: loss=117.43022918701172\n",
      "Batch 550: loss=102.08621978759766\n",
      "Batch 560: loss=79.50537872314453\n",
      "Batch 570: loss=121.91584014892578\n",
      "Batch 580: loss=100.35545349121094\n",
      "Batch 590: loss=79.42054748535156\n",
      "Batch 600: loss=90.94145202636719\n",
      "Batch 610: loss=187.8261260986328\n",
      "Batch 620: loss=88.4120864868164\n",
      "Batch 630: loss=76.55172729492188\n",
      "Batch 640: loss=98.41448211669922\n",
      "Batch 650: loss=115.28980255126953\n",
      "Batch 660: loss=96.57258605957031\n",
      "Batch 670: loss=124.5553970336914\n",
      "Batch 680: loss=57.836669921875\n",
      "Epoch 8\n",
      "Batch 0: loss=97.35962677001953\n",
      "Batch 10: loss=85.68523406982422\n",
      "Batch 20: loss=93.11483764648438\n",
      "Batch 30: loss=105.48551940917969\n",
      "Batch 40: loss=110.80321502685547\n",
      "Batch 50: loss=81.75213623046875\n",
      "Batch 60: loss=134.05348205566406\n",
      "Batch 70: loss=93.1739730834961\n",
      "Batch 80: loss=147.8638916015625\n",
      "Batch 90: loss=108.50497436523438\n",
      "Batch 100: loss=90.41055297851562\n",
      "Batch 110: loss=75.20015716552734\n",
      "Batch 120: loss=86.1368179321289\n",
      "Batch 130: loss=74.50518798828125\n",
      "Batch 140: loss=100.8180923461914\n",
      "Batch 150: loss=168.13250732421875\n",
      "Batch 160: loss=66.3778076171875\n",
      "Batch 170: loss=124.975341796875\n",
      "Batch 180: loss=70.95140838623047\n",
      "Batch 190: loss=163.56971740722656\n",
      "Batch 200: loss=137.34295654296875\n",
      "Batch 210: loss=108.94236755371094\n",
      "Batch 220: loss=117.53759765625\n",
      "Batch 230: loss=95.56173706054688\n",
      "Batch 240: loss=131.8915557861328\n",
      "Batch 250: loss=53.78966522216797\n",
      "Batch 260: loss=80.93916320800781\n",
      "Batch 270: loss=91.37583923339844\n",
      "Batch 280: loss=94.5474624633789\n",
      "Batch 290: loss=92.21185302734375\n",
      "Batch 300: loss=154.8830108642578\n",
      "Batch 310: loss=81.13188171386719\n",
      "Batch 320: loss=100.0172119140625\n",
      "Batch 330: loss=150.8107452392578\n",
      "Batch 340: loss=62.933860778808594\n",
      "Batch 350: loss=114.1867446899414\n",
      "Batch 360: loss=137.06387329101562\n",
      "Batch 370: loss=90.52386474609375\n",
      "Batch 380: loss=67.06825256347656\n",
      "Batch 390: loss=108.95923614501953\n",
      "Batch 400: loss=83.29556274414062\n",
      "Batch 410: loss=122.91131591796875\n",
      "Batch 420: loss=80.73008728027344\n",
      "Batch 430: loss=79.84764099121094\n",
      "Batch 440: loss=95.45465850830078\n",
      "Batch 450: loss=169.92611694335938\n",
      "Batch 460: loss=94.26715850830078\n",
      "Batch 470: loss=179.8067626953125\n",
      "Batch 480: loss=109.4884033203125\n",
      "Batch 490: loss=106.14442443847656\n",
      "Batch 500: loss=76.0072021484375\n",
      "Batch 510: loss=84.60796356201172\n",
      "Batch 520: loss=102.532958984375\n",
      "Batch 530: loss=147.54193115234375\n",
      "Batch 540: loss=117.43023681640625\n",
      "Batch 550: loss=102.08621215820312\n",
      "Batch 560: loss=79.50537872314453\n",
      "Batch 570: loss=121.91584777832031\n",
      "Batch 580: loss=100.35546875\n",
      "Batch 590: loss=79.42054748535156\n",
      "Batch 600: loss=90.94146728515625\n",
      "Batch 610: loss=187.82611083984375\n",
      "Batch 620: loss=88.41209411621094\n",
      "Batch 630: loss=76.55171203613281\n",
      "Batch 640: loss=98.41447448730469\n",
      "Batch 650: loss=115.289794921875\n",
      "Batch 660: loss=96.57258605957031\n",
      "Batch 670: loss=124.55538940429688\n",
      "Batch 680: loss=57.836673736572266\n",
      "Epoch 9\n",
      "Batch 0: loss=97.35962677001953\n",
      "Batch 10: loss=85.68523406982422\n",
      "Batch 20: loss=93.11483764648438\n",
      "Batch 30: loss=105.48551177978516\n",
      "Batch 40: loss=110.80320739746094\n",
      "Batch 50: loss=81.75215148925781\n",
      "Batch 60: loss=134.05348205566406\n",
      "Batch 70: loss=93.17398071289062\n",
      "Batch 80: loss=147.8638916015625\n",
      "Batch 90: loss=108.50497436523438\n",
      "Batch 100: loss=90.41055297851562\n",
      "Batch 110: loss=75.20015716552734\n",
      "Batch 120: loss=86.1368179321289\n",
      "Batch 130: loss=74.50518798828125\n",
      "Batch 140: loss=100.81809997558594\n",
      "Batch 150: loss=168.13250732421875\n",
      "Batch 160: loss=66.37781524658203\n",
      "Batch 170: loss=124.975341796875\n",
      "Batch 180: loss=70.951416015625\n",
      "Batch 190: loss=163.56971740722656\n",
      "Batch 200: loss=137.34295654296875\n",
      "Batch 210: loss=108.94236755371094\n",
      "Batch 220: loss=117.53759765625\n",
      "Batch 230: loss=95.56173706054688\n",
      "Batch 240: loss=131.8915557861328\n",
      "Batch 250: loss=53.78966522216797\n",
      "Batch 260: loss=80.93916320800781\n",
      "Batch 270: loss=91.37583923339844\n",
      "Batch 280: loss=94.54745483398438\n",
      "Batch 290: loss=92.21185302734375\n",
      "Batch 300: loss=154.88299560546875\n",
      "Batch 310: loss=81.13187408447266\n",
      "Batch 320: loss=100.0172119140625\n",
      "Batch 330: loss=150.81072998046875\n",
      "Batch 340: loss=62.933860778808594\n",
      "Batch 350: loss=114.1867446899414\n",
      "Batch 360: loss=137.06387329101562\n",
      "Batch 370: loss=90.52386474609375\n",
      "Batch 380: loss=67.06825256347656\n",
      "Batch 390: loss=108.95924377441406\n",
      "Batch 400: loss=83.29556274414062\n",
      "Batch 410: loss=122.91136169433594\n",
      "Batch 420: loss=80.7300796508789\n",
      "Batch 430: loss=79.84764099121094\n",
      "Batch 440: loss=95.45466613769531\n",
      "Batch 450: loss=169.92611694335938\n",
      "Batch 460: loss=94.26715087890625\n",
      "Batch 470: loss=179.80677795410156\n",
      "Batch 480: loss=109.4884033203125\n",
      "Batch 490: loss=106.14442443847656\n",
      "Batch 500: loss=76.0072021484375\n",
      "Batch 510: loss=84.60795593261719\n",
      "Batch 520: loss=102.532958984375\n",
      "Batch 530: loss=147.54193115234375\n",
      "Batch 540: loss=117.43023681640625\n",
      "Batch 550: loss=102.08621215820312\n",
      "Batch 560: loss=79.50537872314453\n",
      "Batch 570: loss=121.91585540771484\n",
      "Batch 580: loss=100.35546875\n",
      "Batch 590: loss=79.42054748535156\n",
      "Batch 600: loss=90.94147491455078\n",
      "Batch 610: loss=187.82611083984375\n",
      "Batch 620: loss=88.41209411621094\n",
      "Batch 630: loss=76.55171203613281\n",
      "Batch 640: loss=98.41447448730469\n",
      "Batch 650: loss=115.289794921875\n",
      "Batch 660: loss=96.57258605957031\n",
      "Batch 670: loss=124.55538940429688\n",
      "Batch 680: loss=57.83667755126953\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    for epoch in range(0, 10):\n",
    "        print(f'Epoch {epoch}')\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            original_imgs = load_imgs(batch)\n",
    "            orig_l_list, rs_l_tensor, gt = preprocess_imgs(original_imgs)\n",
    "\n",
    "            output_tensor = model(rs_l_tensor.cuda())        \n",
    "\n",
    "            loss = F.mse_loss(output_tensor, gt.cuda())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Batch {batch_idx}: loss={loss}')\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        torch.save(model.state_dict(), f'model_{epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3691103f",
   "metadata": {
    "papermill": {
     "duration": 0.057559,
     "end_time": "2023-07-13T07:09:07.830479",
     "exception": false,
     "start_time": "2023-07-13T07:09:07.772920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 39087.47805,
   "end_time": "2023-07-13T07:09:11.128652",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-12T20:17:43.650602",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
