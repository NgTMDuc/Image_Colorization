{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nimport warnings\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-13T11:58:22.236556Z","iopub.execute_input":"2023-07-13T11:58:22.236976Z","iopub.status.idle":"2023-07-13T11:58:25.294409Z","shell.execute_reply.started":"2023-07-13T11:58:22.236943Z","shell.execute_reply":"2023-07-13T11:58:25.293433Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"images_paths = list()\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        image_path = os.path.join(dirname, filename)\n        if image_path.endswith('.jpg') or image_path.endswith(\".png\"):\n            images_paths.append(image_path)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:58:25.296424Z","iopub.execute_input":"2023-07-13T11:58:25.296997Z","iopub.status.idle":"2023-07-13T11:58:38.278901Z","shell.execute_reply.started":"2023-07-13T11:58:25.296962Z","shell.execute_reply":"2023-07-13T11:58:38.277861Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class ColorizationDataset(Dataset):\n    def __init__(self, paths):\n        self.paths = paths\n\n    def __getitem__(self, idx):\n        return self.paths[idx]\n\n    def __len__(self):\n        return len(self.paths)\n    \n    \ndef make_dataloader(batch_size=16, n_workers=2, pin_memory=True, **kwargs):\n    dataset = ColorizationDataset(**kwargs)\n    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=n_workers,\n                            pin_memory=pin_memory)\n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:58:38.280708Z","iopub.execute_input":"2023-07-13T11:58:38.281443Z","iopub.status.idle":"2023-07-13T11:58:38.288591Z","shell.execute_reply.started":"2023-07-13T11:58:38.281405Z","shell.execute_reply":"2023-07-13T11:58:38.287631Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"np.random.seed(123)\nall_images = images_paths\ntrain_range = int(0.8 * len(all_images))\nrand_idxs = np.random.permutation(len(all_images))\ntrain_idxs = rand_idxs[:train_range] \nval_idxs = rand_idxs[train_range:] \ntrain_paths = [all_images[x] for x in train_idxs]\nval_paths = [all_images[x] for x in val_idxs]\nprint(len(train_paths), len(val_paths))","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:58:38.290539Z","iopub.execute_input":"2023-07-13T11:58:38.291282Z","iopub.status.idle":"2023-07-13T11:58:38.307644Z","shell.execute_reply.started":"2023-07-13T11:58:38.291231Z","shell.execute_reply":"2023-07-13T11:58:38.306735Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"11019 2755\n","output_type":"stream"}]},{"cell_type":"code","source":"train_loader = make_dataloader(paths=train_paths)\nval_loader = make_dataloader(batch_size=32, paths=val_paths)  # INCREASED\nlen(train_loader), len(val_loader)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:58:38.311560Z","iopub.execute_input":"2023-07-13T11:58:38.311903Z","iopub.status.idle":"2023-07-13T11:58:38.322156Z","shell.execute_reply.started":"2023-07-13T11:58:38.311814Z","shell.execute_reply":"2023-07-13T11:58:38.321168Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(689, 87)"},"metadata":{}}]},{"cell_type":"code","source":"def get_name_from_path(path):\n    return path.split('/')[-1]","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:58:38.323536Z","iopub.execute_input":"2023-07-13T11:58:38.324472Z","iopub.status.idle":"2023-07-13T11:58:38.331900Z","shell.execute_reply.started":"2023-07-13T11:58:38.324437Z","shell.execute_reply":"2023-07-13T11:58:38.331001Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def save_grayscale_batch(batch):\n    for img_path in batch:\n        img_rgb = Image.open(img_path)\n        img_gray = img_rgb.convert('L')\n        img_gray.save(f'{get_name_from_path(img_path)}')","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:58:38.334742Z","iopub.execute_input":"2023-07-13T11:58:38.335376Z","iopub.status.idle":"2023-07-13T11:58:38.343240Z","shell.execute_reply.started":"2023-07-13T11:58:38.335343Z","shell.execute_reply":"2023-07-13T11:58:38.342239Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def save_batch(batch):\n    for img_path in batch:\n        img_rgb = Image.open(img_path)\n        img_rgb.save(f'{get_name_from_path(img_path)}')","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:58:38.344793Z","iopub.execute_input":"2023-07-13T11:58:38.345170Z","iopub.status.idle":"2023-07-13T11:58:38.355404Z","shell.execute_reply.started":"2023-07-13T11:58:38.345139Z","shell.execute_reply":"2023-07-13T11:58:38.354417Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def load_grayscale_batch(batch):\n    return [f'{get_name_from_path(img_path)}' for img_path in batch]","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:58:38.358561Z","iopub.execute_input":"2023-07-13T11:58:38.358886Z","iopub.status.idle":"2023-07-13T11:58:38.368935Z","shell.execute_reply.started":"2023-07-13T11:58:38.358849Z","shell.execute_reply":"2023-07-13T11:58:38.368026Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def del_grayscale_batch(batch):\n    for img_path in batch:\n        os.remove(f'{get_name_from_path(img_path)}')","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:58:38.370474Z","iopub.execute_input":"2023-07-13T11:58:38.371078Z","iopub.status.idle":"2023-07-13T11:58:38.379921Z","shell.execute_reply.started":"2023-07-13T11:58:38.371046Z","shell.execute_reply":"2023-07-13T11:58:38.379025Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\nclass BaseColor(nn.Module):\n    def __init__(self):\n        super(BaseColor, self).__init__()\n\n        self.l_cent = 50.\n        self.l_norm = 100.\n        self.ab_norm = 110.\n\n    def normalize_l(self, in_l):\n        return (in_l-self.l_cent)/self.l_norm\n\n    def unnormalize_l(self, in_l):\n        return in_l*self.l_norm + self.l_cent\n\n    def normalize_ab(self, in_ab):\n        return in_ab/self.ab_norm\n\n    def unnormalize_ab(self, in_ab):\n        return in_ab*self.ab_norm\n\n\n\nclass ECCVGenerator(BaseColor):\n    def __init__(self, norm_layer=nn.BatchNorm2d):\n        super(ECCVGenerator, self).__init__()\n\n        model1=[nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=True),]\n        model1+=[nn.ReLU(True),]\n        model1+=[nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=True),]\n        model1+=[nn.ReLU(True),]\n        model1+=[norm_layer(64),]\n\n        model2=[nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=True),]\n        model2+=[nn.ReLU(True),]\n        model2+=[nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1, bias=True),]\n        model2+=[nn.ReLU(True),]\n        model2+=[norm_layer(128),]\n\n        model3=[nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n        model3+=[nn.ReLU(True),]\n        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n        model3+=[nn.ReLU(True),]\n        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1, bias=True),]\n        model3+=[nn.ReLU(True),]\n        model3+=[norm_layer(256),]\n\n        model4=[nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n        model4+=[nn.ReLU(True),]\n        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n        model4+=[nn.ReLU(True),]\n        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n        model4+=[nn.ReLU(True),]\n        model4+=[norm_layer(512),]\n\n        model5=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n        model5+=[nn.ReLU(True),]\n        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n        model5+=[nn.ReLU(True),]\n        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n        model5+=[nn.ReLU(True),]\n        model5+=[norm_layer(512),]\n\n        model6=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n        model6+=[nn.ReLU(True),]\n        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n        model6+=[nn.ReLU(True),]\n        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n        model6+=[nn.ReLU(True),]\n        model6+=[norm_layer(512),]\n\n        model7=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n        model7+=[nn.ReLU(True),]\n        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n        model7+=[nn.ReLU(True),]\n        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n        model7+=[nn.ReLU(True),]\n        model7+=[norm_layer(512),]\n\n        model8=[nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=True),]\n        model8+=[nn.ReLU(True),]\n        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n        model8+=[nn.ReLU(True),]\n        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n        model8+=[nn.ReLU(True),]\n\n        model8+=[nn.Conv2d(256, 313, kernel_size=1, stride=1, padding=0, bias=True),]\n\n        self.model1 = nn.Sequential(*model1)\n        self.model2 = nn.Sequential(*model2)\n        self.model3 = nn.Sequential(*model3)\n        self.model4 = nn.Sequential(*model4)\n        self.model5 = nn.Sequential(*model5)\n        self.model6 = nn.Sequential(*model6)\n        self.model7 = nn.Sequential(*model7)\n        self.model8 = nn.Sequential(*model8)\n\n        self.softmax = nn.Softmax(dim=1)\n        self.model_out = nn.Conv2d(313, 2, kernel_size=1, padding=0, dilation=1, stride=1, bias=False)\n        self.upsample4 = nn.Upsample(scale_factor=4, mode='bilinear')\n\n    def forward(self, input_l):\n        conv1_2 = self.model1(self.normalize_l(input_l))\n        conv2_2 = self.model2(conv1_2)\n        conv3_3 = self.model3(conv2_2)\n        conv4_3 = self.model4(conv3_3)\n        conv5_3 = self.model5(conv4_3)\n        conv6_3 = self.model6(conv5_3)\n        conv7_3 = self.model7(conv6_3)\n        conv8_3 = self.model8(conv7_3)\n        out_reg = self.model_out(self.softmax(conv8_3))\n\n        return self.unnormalize_ab(self.upsample4(out_reg))\n\ndef eccv16(pretrained=True):\n    model = ECCVGenerator()\n    if(pretrained):\n        import torch.utils.model_zoo as model_zoo\n        model.load_state_dict(model_zoo.load_url('https://colorizers.s3.us-east-2.amazonaws.com/colorization_release_v2-9b330a0b.pth',map_location='cpu',check_hash=True))\n    return model\n\n\nfrom PIL import Image\nimport numpy as np\nfrom skimage import color\nimport torch\nimport torch.nn.functional as F\n\n# def load_img(img_path):\ndef load_imgs(img_path_list):\n    out_np_list = [np.asarray(Image.open(img_path).convert('RGB'))\n                   for img_path in img_path_list]\n    \n    # if(out_np.ndim==2):\n    #    out_np = np.tile(out_np[:,:,None],3)\n    for i, out_np in enumerate(out_np_list):\n        if out_np.ndim==2:\n            out_np_list[i] = np.tile(out_np[:,:,None],3)\n    # return out_np\n    return out_np_list\n\ndef resize_img(img, HW=(256,256), resample=3):\n    return np.asarray(Image.fromarray(img).resize((HW[1],HW[0]), resample=resample))\n\n# def preprocess_img(img_rgb_orig, HW=(256,256), resample=3):\ndef preprocess_imgs(img_rgb_orig_list, HW=(256,256), resample=3):\n    # return original size L and resized L as torch Tensors\n    # img_rgb_rs = resize_img(img_rgb_orig, HW=HW, resample=resample)\n    img_rgb_rs_list = [resize_img(img_rgb_orig, HW=HW, resample=resample)\n                       for img_rgb_orig in img_rgb_orig_list]\n\n    # img_lab_orig = color.rgb2lab(img_rgb_orig)\n    # img_lab_rs = color.rgb2lab(img_rgb_rs)\n    img_lab_orig_list = [color.rgb2lab(img_rgb_orig)\n                         for img_rgb_orig in img_rgb_orig_list]\n    img_lab_rs_list = [color.rgb2lab(img_rgb_rs)\n                       for img_rgb_rs in img_rgb_rs_list]\n\n    # img_l_orig = img_lab_orig[:,:,0]\n    # img_l_rs = img_lab_rs[:,:,0]\n    img_l_orig_list = [img_lab_orig[:,:,0]\n                       for img_lab_orig in img_lab_orig_list]\n    img_l_rs_list = [img_lab_rs[:,:,0]\n                     for img_lab_rs in img_lab_rs_list]\n    # FOR GT\n    # (256, 256, 2) 1 2 -> 0 1\n    img_ab_rs_list = [torch.transpose(torch.transpose(torch.Tensor(img_lab_rs[:,:,1:3]), 1, 2), 0, 1)\n                     for img_lab_rs in img_lab_rs_list]\n\n    # RESHAPE FIRST\n    img_l_rs_list_reshaped = [torch.Tensor(img_l_rs)[None, None, :, :]\n                              for img_l_rs in img_l_rs_list]\n    img_ab_rs_list_reshaped = [torch.Tensor(img_ab_rs)[None,:, :, :]\n                              for img_ab_rs in img_ab_rs_list]\n\n    # tens_orig_l = torch.Tensor(img_l_orig)[None,None,:,:]\n    # tens_rs_l = torch.Tensor(img_l_rs)[None,None,:,:]\n    tens_orig_l_list = [torch.Tensor(img_l_orig)[None,None,:,:]\n                        for img_l_orig in img_l_orig_list]\n    \n    batch_size = len(img_rgb_orig_list)\n    tens_rs_l_tensor = torch.cat(img_l_rs_list_reshaped, dim=0)\n    tens_rs_ab_tensor = torch.cat(img_ab_rs_list_reshaped, dim=0)\n\n    return (tens_orig_l_list, tens_rs_l_tensor, tens_rs_ab_tensor)\n\n# def postprocess_tens(tens_orig_l, out_ab, mode='bilinear'):\ndef postprocess_imgs(tens_orig_l_list, out_ab, mode='bilinear'):\n    # tens_orig_l     1 x 1 x H_orig x W_orig\n    # out_ab         1 x 2 x H x W\n    \n    # tens_orig_l_list: (16 elements) 1 x 1 x H_orig x W_orig\n    # out_ab                          16 x 2 x H x W\n\n    # HW_orig = tens_orig_l.shape[2:]\n    # HW = out_ab.shape[2:]\n    HW_orig_list = [tens_orig_l.shape[2:] for tens_orig_l in tens_orig_l_list]  # 16 ele H_orig x W_orig\n    HW = out_ab.shape[2:]  # (H, W)\n\n    # call resize function if needed\n    #if(HW_orig[0]!=HW[0] or HW_orig[1]!=HW[1]):\n    #    out_ab_orig = F.interpolate(out_ab, size=HW_orig, mode='bilinear')\n    #else:\n    #    out_ab_orig = out_ab\n    \n    out_ab_orig_list = list()\n    for i, HW_orig in enumerate(HW_orig_list):\n        if(HW_orig[0]!=HW[0] or HW_orig[1]!=HW[1]):\n            out_ab_orig = F.interpolate(out_ab[i][None, :, :, :], size=HW_orig, mode='bilinear')\n        else:\n            out_ab_orig = out_ab[i][None, :, :, :]     \n        out_ab_orig_list.append(out_ab_orig)\n\n    # out_lab_orig = torch.cat((tens_orig_l, out_ab_orig), dim=1)\n    out_lab_orig_list = list()\n    for i in range(len(out_ab_orig_list)):\n        out_lab_orig_list.append(\n            torch.cat((tens_orig_l_list[i].cuda(), out_ab_orig_list[i]), dim=1)\n        )\n    \n    # return color.lab2rgb(out_lab_orig.data.cpu().numpy()[0,...].transpose((1,2,0)))\n    return [color.lab2rgb(out_lab_orig.data.cpu().numpy()[0,...].transpose((1,2,0)))\n           for out_lab_orig in out_lab_orig_list]","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:58:38.383467Z","iopub.execute_input":"2023-07-13T11:58:38.383791Z","iopub.status.idle":"2023-07-13T11:58:38.449604Z","shell.execute_reply.started":"2023-07-13T11:58:38.383764Z","shell.execute_reply":"2023-07-13T11:58:38.448770Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model = eccv16(pretrained=False).cuda()\nmodel.load_state_dict(torch.load('/kaggle/input/fork-of-eccv16-train/model_9'))\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:58:38.451030Z","iopub.execute_input":"2023-07-13T11:58:38.451375Z","iopub.status.idle":"2023-07-13T11:58:43.532333Z","shell.execute_reply.started":"2023-07-13T11:58:38.451343Z","shell.execute_reply":"2023-07-13T11:58:43.531282Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"ECCVGenerator(\n  (model1): Sequential(\n    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (model2): Sequential(\n    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (model3): Sequential(\n    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (5): ReLU(inplace=True)\n    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (model4): Sequential(\n    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): ReLU(inplace=True)\n    (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (model5): Sequential(\n    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n    (3): ReLU(inplace=True)\n    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n    (5): ReLU(inplace=True)\n    (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (model6): Sequential(\n    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n    (3): ReLU(inplace=True)\n    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n    (5): ReLU(inplace=True)\n    (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (model7): Sequential(\n    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): ReLU(inplace=True)\n    (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (model8): Sequential(\n    (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): ReLU(inplace=True)\n    (6): Conv2d(256, 313, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (softmax): Softmax(dim=1)\n  (model_out): Conv2d(313, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (upsample4): Upsample(scale_factor=4.0, mode='bilinear')\n)"},"metadata":{}}]},{"cell_type":"code","source":"import math\nfrom PIL import Image\nimport cv2\nfrom cv2 import imshow\n\ndef gaussian(window_size, sigma):\n    gauss = torch.Tensor([math.exp(-(x - window_size//2) ** 2/float(2*sigma**2)) for x in range(window_size)])\n    return gauss/gauss.sum()\n\n\ndef create_window(window_size, channel=1):\n\n    # Generate an 1D tensor containing values sampled from a gaussian distribution\n    _1d_window = gaussian(window_size=window_size, sigma=1.5).unsqueeze(1)\n    \n    # Converting to 2D  \n    _2d_window = _1d_window.mm(_1d_window.t()).float().unsqueeze(0).unsqueeze(0)\n     \n    window = torch.Tensor(_2d_window.expand(channel, 1, window_size, window_size).contiguous())\n\n    return window\n\n\ndef ssim(img1, img2, val_range, window_size=11, window=None, size_average=True, full=False):\n\n    L = val_range # L is the dynamic range of the pixel values (255 for 8-bit grayscale images),\n\n    pad = window_size // 2\n    \n    try:\n        _, channels, height, width = img1.size()\n    except:\n        channels, height, width = img1.size()\n\n    # if window is not provided, init one\n    if window is None: \n        real_size = min(window_size, height, width) # window should be atleast 11x11 \n        window = create_window(real_size, channel=channels).to(img1.device)\n    \n    # calculating the mu parameter (locally) for both images using a gaussian filter \n    # calculates the luminosity params\n    mu1 = F.conv2d(img1, window, padding=pad, groups=channels)\n    mu2 = F.conv2d(img2, window, padding=pad, groups=channels)\n    \n    mu1_sq = mu1 ** 2\n    mu2_sq = mu2 ** 2 \n    mu12 = mu1 * mu2\n\n    # now we calculate the sigma square parameter\n    # Sigma deals with the contrast component \n    sigma1_sq = F.conv2d(img1 * img1, window, padding=pad, groups=channels) - mu1_sq\n    sigma2_sq = F.conv2d(img2 * img2, window, padding=pad, groups=channels) - mu2_sq\n    sigma12 =  F.conv2d(img1 * img2, window, padding=pad, groups=channels) - mu12\n\n    # Some constants for stability \n    C1 = (0.01 * 255) ** 2  # NOTE: Removed L from here (ref PT implementation)\n    C2 = (0.03 * 255) ** 2 \n\n    contrast_metric = (2.0 * sigma12 + C2) / (sigma1_sq + sigma2_sq + C2)\n    contrast_metric = torch.mean(contrast_metric)\n\n    numerator1 = 2 * mu12 + C1  \n    numerator2 = 2 * sigma12 + C2\n    denominator1 = mu1_sq + mu2_sq + C1 \n    denominator2 = sigma1_sq + sigma2_sq + C2\n\n    ssim_score = (numerator1 * numerator2) / (denominator1 * denominator2)\n\n    if size_average:\n        ret = ssim_score.mean() \n    else: \n        ret = ssim_score.mean(1).mean(1).mean(1)\n    \n    if full:\n        return ret, contrast_metric\n    \n    return ret\n\n\n# display imgs \ndef display_imgs(x, transpose=True, resize=True):\n    if resize:\n        x=cv2.resize(x, (400, 400))\n    if transpose:\n        imshow(cv2.cvtColor(x, cv2.COLOR_BGR2RGB))\n    else:\n        imshow(x)\n\ndef tensorify(x, mul = True):\n#   if vector is already norm, set mul = False\n    if mul:\n        return torch.Tensor(x.transpose((2, 0, 1))).unsqueeze(0).float().div(255.0)\n    else:\n        return torch.Tensor(x.transpose((2, 0, 1))).unsqueeze(0).float()\n\ndef compute_ssim(image1, image2):\n    img1 = tensorify(np.asarray(image1))\n    img2 = tensorify(np.asarray(image2))\n    return ssim(img1, img2, val_range= 255)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:58:43.534801Z","iopub.execute_input":"2023-07-13T11:58:43.535204Z","iopub.status.idle":"2023-07-13T11:58:43.736099Z","shell.execute_reply.started":"2023-07-13T11:58:43.535167Z","shell.execute_reply":"2023-07-13T11:58:43.735179Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import gc","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:58:43.739971Z","iopub.execute_input":"2023-07-13T11:58:43.740254Z","iopub.status.idle":"2023-07-13T11:58:43.744236Z","shell.execute_reply.started":"2023-07-13T11:58:43.740229Z","shell.execute_reply":"2023-07-13T11:58:43.743341Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"all_scores = list()\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n\n    for batch_idx, batch in enumerate(val_loader):\n        original_imgs = load_imgs(batch)\n        orig_l_list, rs_l_tensor, gt = preprocess_imgs(original_imgs)\n\n        output_tensor = model(rs_l_tensor.cuda()) \n\n        \n        postprocessed_imgs = postprocess_imgs(orig_l_list, output_tensor)\n\n        scores = [compute_ssim(postprocessed_imgs[i]*255, np.asarray(Image.open(batch[i]).convert(\"RGB\")))\n                  for i in range(len(batch))]\n        all_scores.extend(scores)\n        print(np.mean(scores), '\\t', f'Current accumulative score={np.mean(all_scores):.15f}')\n        \n        gc.collect()\n        torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-07-13T11:58:43.745821Z","iopub.execute_input":"2023-07-13T11:58:43.746480Z","iopub.status.idle":"2023-07-13T12:01:45.726858Z","shell.execute_reply.started":"2023-07-13T11:58:43.746447Z","shell.execute_reply":"2023-07-13T12:01:45.725534Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"0.99950624 \t Current accumulative score=0.999506235122681\n0.9992124 \t Current accumulative score=0.999359250068665\n0.99949014 \t Current accumulative score=0.999402940273285\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m output_tensor \u001b[38;5;241m=\u001b[39m model(rs_l_tensor\u001b[38;5;241m.\u001b[39mcuda()) \n\u001b[1;32m     13\u001b[0m postprocessed_imgs \u001b[38;5;241m=\u001b[39m postprocess_imgs(orig_l_list, output_tensor)\n\u001b[0;32m---> 15\u001b[0m scores \u001b[38;5;241m=\u001b[39m [compute_ssim(postprocessed_imgs[i]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m255\u001b[39m, np\u001b[38;5;241m.\u001b[39masarray(Image\u001b[38;5;241m.\u001b[39mopen(batch[i])\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m     16\u001b[0m           \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch))]\n\u001b[1;32m     17\u001b[0m all_scores\u001b[38;5;241m.\u001b[39mextend(scores)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(scores), \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCurrent accumulative score=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(all_scores)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.15f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","Cell \u001b[0;32mIn[16], line 15\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m output_tensor \u001b[38;5;241m=\u001b[39m model(rs_l_tensor\u001b[38;5;241m.\u001b[39mcuda()) \n\u001b[1;32m     13\u001b[0m postprocessed_imgs \u001b[38;5;241m=\u001b[39m postprocess_imgs(orig_l_list, output_tensor)\n\u001b[0;32m---> 15\u001b[0m scores \u001b[38;5;241m=\u001b[39m [\u001b[43mcompute_ssim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpostprocessed_imgs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m           \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch))]\n\u001b[1;32m     17\u001b[0m all_scores\u001b[38;5;241m.\u001b[39mextend(scores)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(scores), \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCurrent accumulative score=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(all_scores)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.15f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","Cell \u001b[0;32mIn[14], line 99\u001b[0m, in \u001b[0;36mcompute_ssim\u001b[0;34m(image1, image2)\u001b[0m\n\u001b[1;32m     97\u001b[0m img1 \u001b[38;5;241m=\u001b[39m tensorify(np\u001b[38;5;241m.\u001b[39masarray(image1))\n\u001b[1;32m     98\u001b[0m img2 \u001b[38;5;241m=\u001b[39m tensorify(np\u001b[38;5;241m.\u001b[39masarray(image2))\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[14], line 43\u001b[0m, in \u001b[0;36mssim\u001b[0;34m(img1, img2, val_range, window_size, window, size_average, full)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# calculating the mu parameter (locally) for both images using a gaussian filter \u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# calculates the luminosity params\u001b[39;00m\n\u001b[1;32m     42\u001b[0m mu1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mconv2d(img1, window, padding\u001b[38;5;241m=\u001b[39mpad, groups\u001b[38;5;241m=\u001b[39mchannels)\n\u001b[0;32m---> 43\u001b[0m mu2 \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchannels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m mu1_sq \u001b[38;5;241m=\u001b[39m mu1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     46\u001b[0m mu2_sq \u001b[38;5;241m=\u001b[39m mu2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}