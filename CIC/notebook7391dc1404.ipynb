{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T19:59:08.414172Z","iopub.status.busy":"2023-07-12T19:59:08.413778Z","iopub.status.idle":"2023-07-12T19:59:08.425222Z","shell.execute_reply":"2023-07-12T19:59:08.424079Z","shell.execute_reply.started":"2023-07-12T19:59:08.414136Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","\n","\"\"\"# Input Image Dimension\"\"\"\n","\n","# Change the dimension below if you want to change the input dimension\n","# if you are changing the dimension than be carefull to tweak the model\n","# architecture in model.py\n","\n","HEIGHT = 256\n","WIDTH  = 256\n","\n","\"\"\"# Model\"\"\"\n","\n","# Don't Change kernel size if you are not sure about what it is\n","# basically this is the Size of Convolution kernels\n","# In the original implementation of 'Let There be Colour' kernel size was kept\n","# as (3,3) so I have left it as it was in original\n","\n","KERNEL_SIZE = (3,3) \n","\n","# I tried using multiple kinds of activation functions but found that sigmoid\n","# turns out to be a better choice initially I started with tanh activation than\n","# noticed that output images are becoming more red also since tanh has a range\n","# of -1 to 1 it is not good for final output layer if your input images are \n","# scaled between 0 to 1\n","  \n","ACTIVATION_FUNCTION = 'sigmoid'\n","\n","# Loss Functions that I tried were MSE , MASE, and MAE. I found that MSE is \n","# better that or equal to all other in results So I have kept it below although\n","# MASE will also produce similar results \n","\n","LOSS_FUNCTION = tf.keras.losses.MeanSquaredError()\n","\n","LEARNING_RATE = 1e-3\n","\n","\"\"\"# Paths\"\"\"\n","\n","TRAIN_DIR_PATH   = \"/kaggle/input/ukraine-images-2023\"\n","TEST_DIR_PATH    = \"\"\n","VAL_DIR_PATH     = \"\"\n","SAVE_MODEL_PATH  = \"\"\n","SAVE_OUTPUT_PATH = \"\"\n","LOAD_MODEL_PATH  = \"\"\n","SAVE_CSV_PATH    = \"\"\n","LOAD_CSV_PATH    = \"\"\n","\n","\"\"\"# other \"\"\"\n","\n","# NUMBER_OF_TRAINING_EXAMPLES = 500\n","NUMBER_OF_TRAINING_EXAMPLES = 20\n","NUMBER_OF_TEST_EXAMPLES     = 40\n","NUMBER_OF_VAL_EXAMPLES      = 50\n","# NUMBER_OF_EPOCHS            = 100\n","NUMBER_OF_EPOCHS            = 1\n","STEPS_PER_EPOCHS            = 100\n","VALIDATION_STEPS            = 20\n","BATCH_SIZE                  = 50"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T19:59:08.427844Z","iopub.status.busy":"2023-07-12T19:59:08.427458Z","iopub.status.idle":"2023-07-12T19:59:08.454515Z","shell.execute_reply":"2023-07-12T19:59:08.453496Z","shell.execute_reply.started":"2023-07-12T19:59:08.427795Z"},"trusted":true},"outputs":[],"source":["\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from skimage.color import rgb2lab, lab2rgb\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n","\n","\n","def loadImagesToArray(dir_path, num_of_img=-1, search_inside =False):\n","  \"\"\"\n","  dir_path : path of directory from which images will be imported\n","  num_of_imgs (Integer): number of images to be imported from the directory if not \n","              given than all images will be imported \n","  search_inside (boolean, default : False) : If true all images inside that directory\n","              along with the images in subdirectory will be added to output array\n","  \"\"\"\n","  images = []\n","  count = -1\n","  if search_inside==False:\n","      for filename in os.listdir(dir_path):\n","          count+=1\n","          if(count==num_of_img):\n","              break\n","          images.append(img_to_array(load_img(dir_path+os.sep+filename).resize((256, 256), resample=3)))\n","  if search_inside==True:\n","      for root,dirs,files in os.walk(dir_path):\n","        for filename in files:\n","            count+=1\n","            if(count==num_of_img):\n","                break\n","            images.append(img_to_array(load_img(root+os.sep+filename).resize((256, 256), resample=3)))\n","  # print([img.shape for img in images])\n","  return np.array(images,dtype=float)/255.0\n","\n","def DataGenerator():\n","    DataGen = ImageDataGenerator(        \n","        shear_range=0.2,\n","        zoom_range=0.2,\n","        rotation_range=20,\n","        horizontal_flip=True)\n","    return DataGen\n","\n","def RGB2GRAY(img,add_channel_dim=False):\n","  conv_matrix = np.array([0.212671 ,0.715160,0.072169])\n","  gray_img = img @ conv_matrix\n","  if add_channel_dim==True:\n","    return gray_img.reshape(np.array([*list(gray_img.shape),1]))\n","  else:\n","    return gray_img\n","\n","def RGB2ab(img,use_skimage=True):\n","  \"\"\"\n","  Refrences\n","  * https://en.wikipedia.org/wiki/Lab_color_space\n","  * https://github.com/scikit-image/scikit-image/blob/main/skimage/color/colorconv.py#L990-L1050\n","  \"\"\"\n","  if use_skimage==False:\n","    def finv(cie):\n","      cond = cie > 0.008856\n","      cie[cond] = np.cbrt(cie[cond])\n","      cie[~cond] = 7.787 * cie[~cond] + 16. / 116.\n","      return cie     \n","\n","    conv_matrix =np.array( [[0.412453, 0.357580, 0.180423],\n","            [0.212671, 0.715160, 0.072169],\n","            [0.019334, 0.119193, 0.950227]])\n","    CIE = np.matmul(img,conv_matrix.T)\n","    CIE[0] = CIE[0]/0.95047\n","    CIE[2] = CIE[2]/1.08883\n","    CIE = finv(CIE)\n","    x, y, z = CIE[..., 0], CIE[..., 1], CIE[..., 2]\n","    a =  (500*(x-y)+127)/255.0\n","    b =  (200*(y-z)+127)/255.0\n","    return np.concatenate([x[..., np.newaxis] for x in [a, b]], axis=-1)\n","  else:\n","    Lab = rgb2lab(img)\n","    a = (Lab[...,1]+127)/255.0\n","    b = (Lab[...,2]+127)/255.0\n","    return np.concatenate([x[..., np.newaxis] for x in [a, b]], axis=-1)\n","\n","def Lab2RGB(gray,ab):\n","  \"\"\"\n","    Parameters\n","    ----------\n","    gray : nd array\n","        lumminnance component of a image.\n","    ab : TYPE\n","        a and b componenets of a CIE L*a*b image.\n","\n","    Returns\n","    -------\n","    ndarray with R G B components.\n","  \"\"\"\n","  ab = ab*255.0 -127\n","  gray = gray*100\n","  Lab =np.concatenate([x[..., np.newaxis] for x in [gray[...,0], ab[...,0],ab[...,1]]], axis=-1)\n","  return lab2rgb(Lab)\n","\n","def compare_results(img_gt,img_in,img_out,save_results=False,save_as=\"\"):\n","  \"\"\"\n","    Parameters\n","    ----------\n","    img_gt : ndarray with RGB components \n","        Original Required image model is expected to produce this as ouput.\n","    img_in : grayscaled ndarray\n","        image used as input to the model.\n","    img_out : nd array with RGB componets.\n","        The ouput from the model.\n","    save_results : boolean, optional\n","        If True matplotlib.plt will be used to save model. The default is False.\n","    save_as : String, optional\n","        Output file name along with path. The default is \"\".\n","\n","    Returns\n","    -------\n","    None.\n","\n","  \"\"\"\n","  fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n","  ax1.imshow(img_gt)\n","  ax1.set_title('Ground Truth')\n","  ax2.imshow(img_in,cmap='gray')\n","  ax2.set_title('Input')\n","  ax3.imshow(img_out)\n","  ax3.set_title('Output')\n","  axes = [ax1,ax2,ax3]\n","  for ax in axes:\n","    ax.set_xticks([])\n","    ax.set_yticks([])\n","  plt.show()\n","  if save_results==True:\n","    path = save_as+'.svg'\n","    fig.savefig(path,dpi=300)\n","\n","def BatchGenerator(data,imgDataGen,batch_size=64):\n","  for batch in imgDataGen.flow(data, batch_size=batch_size):\n","    yield RGB2GRAY(batch,True), RGB2ab(batch)\n","\n","\n","    "]},{"cell_type":"code","execution_count":27,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-12T19:59:08.459564Z","iopub.status.busy":"2023-07-12T19:59:08.455751Z","iopub.status.idle":"2023-07-12T19:59:08.483312Z","shell.execute_reply":"2023-07-12T19:59:08.482043Z","shell.execute_reply.started":"2023-07-12T19:59:08.459535Z"},"trusted":true},"outputs":[],"source":["\n","\n","import tensorflow as tf\n","# import config\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Conv2D, Conv2DTranspose, RepeatVector,Reshape,Dense, Flatten, Input, Concatenate\n","\n","\n","def build_model(ks=(3,3),act='sigmoid',learning_rate=1e-2):\n","    \n","  # Input Layer\n","  input_lvl = Input(shape = (HEIGHT,WIDTH,1))\n","  \n","  # Initial Shared Network of Low - Level Features\n","  low_lvl = Conv2D(64 ,kernel_size=ks,strides=(2,2),activation=act,padding='SAME')(input_lvl)\n","  low_lvl = layers.BatchNormalization()(low_lvl)\n","  low_lvl = Conv2D(128,kernel_size=ks,strides=(1,1),activation=act,padding='SAME')(low_lvl) \n","  low_lvl = layers.BatchNormalization()(low_lvl)\n","  low_lvl = Conv2D(128,kernel_size=ks,strides=(2,2),activation=act,padding='SAME')(low_lvl) \n","  low_lvl = layers.BatchNormalization()(low_lvl)\n","  low_lvl = Conv2D(256,kernel_size=ks,strides=(1,1),activation=act,padding='SAME')(low_lvl) \n","  low_lvl = layers.BatchNormalization()(low_lvl)\n","  low_lvl = Conv2D(256,kernel_size=ks,strides=(2,2),activation=act,padding='SAME')(low_lvl)\n","  low_lvl = layers.BatchNormalization()(low_lvl)\n","  low_lvl = Conv2D(512,kernel_size=ks,strides=(1,1),activation=act,padding='SAME')(low_lvl)\n","  low_lvl = layers.BatchNormalization()(low_lvl)\n","\n","  # Path one for  Mid-Level Features Network\n","  mid_lvl = Conv2D(512,kernel_size=ks,strides=(1,1),activation=act,padding='SAME')(low_lvl)\n","  mid_lvl = layers.BatchNormalization()(mid_lvl)\n","  mid_lvl = Conv2D(256,kernel_size=ks,strides=(1,1),activation=act,padding='SAME')(mid_lvl)\n","  mid_lvl = layers.BatchNormalization()(mid_lvl)\n","\n","  # Path two for Global Features Network\n","  global_lvl = Conv2D(512,kernel_size=ks,strides=(2,2),activation=act,padding='SAME')(low_lvl)\n","  global_lvl = layers.BatchNormalization()(global_lvl)\n","  global_lvl = Conv2D(512,kernel_size=ks,strides=(1,1),activation=act,padding='SAME')(global_lvl)\n","  global_lvl = layers.BatchNormalization()(global_lvl)\n","  global_lvl = Conv2D(512,kernel_size=ks,strides=(2,2),activation=act,padding='SAME')(global_lvl)\n","  global_lvl = layers.BatchNormalization()(global_lvl)\n","  global_lvl = Conv2D(512,kernel_size=ks,strides=(1,1),activation=act,padding='SAME')(global_lvl)\n","  global_lvl = layers.BatchNormalization()(global_lvl)\n","  global_lvl = Flatten()(global_lvl) \n","  global_lvl = Dense(1024,activation=act)(global_lvl)\n","  global_lvl = Dense(512 ,activation=act)(global_lvl)\n","  global_lvl = Dense(256 ,activation=act)(global_lvl)\n","  \n","\n","  # Fusing the output of above two paths\n","  fusion_lvl = RepeatVector(mid_lvl.shape[1] * mid_lvl.shape[1])(global_lvl) \n","  fusion_lvl = Reshape(([mid_lvl.shape[1],mid_lvl.shape[1]  , 256]))(fusion_lvl)\n","  fusion_lvl = Concatenate( axis=3)([mid_lvl, fusion_lvl]) \n","  fusion_lvl = Conv2D(256, kernel_size=ks,strides =(1, 1), activation=act,padding='SAME')(fusion_lvl)\n","\n","  # Colorization Network\n","  # Instead of UpSampling Layers I am using 2D Convolutional Transpose or deconv for upscaling the images \n","  color_lvl = Conv2DTranspose(128,kernel_size = ks,strides = (1,1),padding='SAME',activation=act)(fusion_lvl)\n","  color_lvl = layers.BatchNormalization()(color_lvl)\n","  color_lvl = Conv2DTranspose(64,kernel_size = ks,strides = (2,2),padding='SAME',activation=act)(color_lvl)\n","  color_lvl = layers.BatchNormalization()(color_lvl)\n","  color_lvl = Conv2DTranspose(64,kernel_size = ks,strides = (1,1),padding='SAME',activation=act)(color_lvl)\n","  color_lvl = layers.BatchNormalization()(color_lvl)\n","  color_lvl = Conv2DTranspose(32,kernel_size = ks,strides = (2,2),padding='SAME',activation=act)(color_lvl)\n","  color_lvl = layers.BatchNormalization()(color_lvl)\n","  \n","  # I added the below mentioned two lines when I trained the model for 100 X 100 sized images\n","  # Ignore if you are using 256 X 256\n","  # color_lvl = Conv2D(32,kernel_size = ks,strides = (1,1),padding='VALID',activation=act)(color_lvl)\n","  # color_lvl = layers.BatchNormalization()(color_lvl)\n","\n","  # Output Layer\n","  output_lvl = Conv2DTranspose(2,kernel_size=ks,strides=(2,2),padding='SAME',activation='sigmoid')(color_lvl)\n","\n","\n","  # Model Parameters\n","  model = Model(inputs = input_lvl, outputs = output_lvl)\n","  optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","  model.compile(\n","      loss = LOSS_FUNCTION,\n","      optimizer = optimizer,\n","      metrics = ['accuracy',\n","          tf.keras.metrics.CosineSimilarity()\n","          ])\n","  return model"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T19:59:08.545019Z","iopub.status.busy":"2023-07-12T19:59:08.543312Z","iopub.status.idle":"2023-07-12T20:00:38.141067Z","shell.execute_reply":"2023-07-12T20:00:38.139834Z","shell.execute_reply.started":"2023-07-12T19:59:08.544986Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(20, 256, 256, 3)\n","100/100 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 0.5026 - cosine_similarity: 0.9979"]},{"ename":"ValueError","evalue":"('Input data in `NumpyArrayIterator` should have rank 4. You passed an array with shape', (0,))","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[28], line 48\u001b[0m\n\u001b[1;32m     44\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [log]\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 48\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mcolorize_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mBatchGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgDataGen\u001b[49m\u001b[43m,\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBatchGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalimgDataGen\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVALIDATION_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTEPS_PER_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUMBER_OF_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#Plotting and saving the history\u001b[39;00m\n\u001b[1;32m     58\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(history\u001b[38;5;241m.\u001b[39mhistory)\u001b[38;5;241m.\u001b[39mplot(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","Cell \u001b[0;32mIn[26], line 141\u001b[0m, in \u001b[0;36mBatchGenerator\u001b[0;34m(data, imgDataGen, batch_size)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mBatchGenerator\u001b[39m(data,imgDataGen,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m):\n\u001b[0;32m--> 141\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mimgDataGen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m RGB2GRAY(batch,\u001b[38;5;28;01mTrue\u001b[39;00m), RGB2ab(batch)\n","\u001b[0;31mValueError\u001b[0m: ('Input data in `NumpyArrayIterator` should have rank 4. You passed an array with shape', (0,))"]}],"source":["\n","import os\n","# import config\n","# import tools\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","# import modelArchitecture as net\n","\n","\n","train_data = loadImagesToArray(\n","    dir_path   = TRAIN_DIR_PATH,\n","    num_of_img = NUMBER_OF_TRAINING_EXAMPLES,\n","    search_inside = True)\n","\n","val_data = loadImagesToArray(\n","    dir_path   = VAL_DIR_PATH,\n","    num_of_img = NUMBER_OF_VAL_EXAMPLES,\n","    search_inside = True)\n","\n","\n","imgDataGen = DataGenerator()\n","valimgDataGen = DataGenerator()\n","\n","\"\"\"# Training \"\"\"\n","colorize_model = build_model(\n","    ks = KERNEL_SIZE,\n","    act=ACTIVATION_FUNCTION,\n","    learning_rate=LEARNING_RATE)\n","\n","# Load Previously trained model\n","try:\n","    colorize_model.load_weights(LOAD_MODEL_PATH)\n","except:\n","    pass\n","\n","log = tf.keras.callbacks.CSVLogger(SAVE_CSV_PATH,append=True, separator=',')\n","callbacks = [log]\n","\n","print(train_data.shape)\n","\n","history = colorize_model.fit(\n","    BatchGenerator(train_data, imgDataGen,BATCH_SIZE),\n","    validation_data = BatchGenerator(val_data, valimgDataGen),\n","    validation_steps=VALIDATION_STEPS,\n","    steps_per_epoch =STEPS_PER_EPOCHS,\n","    epochs=NUMBER_OF_EPOCHS,\n","    callbacks=callbacks)\n","\n","\n","#Plotting and saving the history\n","pd.DataFrame(history.history).plot(figsize=(8,5))\n","plt.show()\n","\n","\n","colorize_model.save(SAVE_MODEL_PATH)\n","\n","\"\"\"# Testing \"\"\"\n","test_images = loadImagesToArray(\n","    dir_path = TEST_DIR_PATH,\n","    num_of_img = NUMBER_OF_TEST_EXAMPLES,\n","    search_inside=True)\n","\n","gray = RGB2GRAY(test_images,True)\n","gray2 = RGB2GRAY(test_images)\n","pred = colorize_model.predict(gray)\n","\n","for i in range(NUMBER_OF_TEST_EXAMPLES):\n","    output = Lab2RGB(gray[i],pred[i])\n","    path =  SAVE_OUTPUT_PATH+os.sep+\"img_\"+str(i)\n","    compare_results(test_images[i],gray2[i],output.reshape(test_images[i].shape),save_results=True,save_as=path)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
